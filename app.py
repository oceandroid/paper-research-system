"""
Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå…¨ä½“å‚¾å‘è¦ç´„æ©Ÿèƒ½è¿½åŠ ç‰ˆï¼‰
- å€‹åˆ¥è«–æ–‡è¦ç´„ â†’ å…¨ä½“å‚¾å‘åˆ†æã«å¤‰æ›´
- Semantic Scholar Rate limitå¯¾ç­–
- Gemini APIæœ€æ–°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
"""
import streamlit as st
import sys
import os
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib
import pandas as pd
from collections import Counter
import time
import requests
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET
import re
from itertools import combinations
import networkx as nx
import json

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“š",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if 'papers' not in st.session_state:
    st.session_state.papers = []
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ''


# ==================== PubMed Crawler ====================
class PubMedCrawler:
    """PubMed APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self, email: str = "user@example.com"):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = email

    def search_papers(self, keyword: str, max_results: int = 20, year_from: Optional[int] = None) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_term = keyword
            if year_from:
                search_term = f"{keyword} AND {year_from}[PDAT]:{datetime.now().year}[PDAT]"

            search_params = {
                'db': 'pubmed',
                'term': search_term,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email
            }

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()
            id_list = search_data.get('esearchresult', {}).get('idlist', [])

            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            batch_size = 20
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                ids_str = ','.join(batch_ids)

                fetch_params = {'db': 'pubmed', 'id': ids_str, 'retmode': 'xml', 'email': self.email}
                fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
                fetch_response.raise_for_status()
                root = ET.fromstring(fetch_response.content)

                for article in root.findall('.//PubmedArticle'):
                    try:
                        paper_info = self._extract_paper_info(article, keyword)
                        papers.append(paper_info)
                    except:
                        continue
                time.sleep(0.5)

            return papers

        except Exception as e:
            st.error(f"PubMed API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def _extract_paper_info(self, article_xml, keyword: str) -> Dict:
        article = article_xml.find('.//Article')
        title = article.findtext('.//ArticleTitle', 'N/A')

        authors = []
        for author in article.findall('.//Author'):
            lastname = author.findtext('LastName', '')
            forename = author.findtext('ForeName', '')
            if lastname:
                authors.append(f"{forename} {lastname}".strip())

        pub_date = article.find('.//PubDate')
        year = 'N/A'
        if pub_date is not None:
            year = pub_date.findtext('Year', 'N/A')

        abstract_texts = article.findall('.//AbstractText')
        abstract = ' '.join([a.text for a in abstract_texts if a.text]) if abstract_texts else 'N/A'

        venue = article.findtext('.//Journal/Title', 'N/A')
        pmid = article_xml.findtext('.//PMID', 'N/A')
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else 'N/A'

        return {
            'title': title, 'authors': authors, 'year': year, 'abstract': abstract,
            'venue': venue, 'url': url, 'pmid': pmid, 'citations': 0,
            'crawled_at': datetime.now().isoformat(), 'keyword': keyword, 'source': 'PubMed'
        }

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed', 'term': keyword, 'retmax': max_results,
                'retmode': 'json', 'email': self.email, 'sort': 'date', 'reldate': days
            }

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()
            id_list = search_data.get('esearchresult', {}).get('idlist', [])

            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            ids_str = ','.join(id_list)
            fetch_params = {'db': 'pubmed', 'id': ids_str, 'retmode': 'xml', 'email': self.email}
            fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.content)

            for article in root.findall('.//PubmedArticle'):
                try:
                    paper_info = self._extract_paper_info(article, keyword)
                    papers.append(paper_info)
                except:
                    continue

            return papers

        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers


# ==================== Semantic Scholar Crawler ====================
class SemanticScholarCrawler:
    """Semantic Scholar APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—ï¼ˆRate limitå¯¾ç­–ç‰ˆï¼‰"""

    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.headers = {'User-Agent': 'Mozilla/5.0'}

    def search_papers(self, keyword: str, max_results: int = 20, year_from: Optional[int] = None) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}/paper/search"
            limit_per_request = min(max_results, 100)  # Semantic Scholarã¯100ä»¶ã¾ã§å¯¾å¿œ
            params = {
                'query': keyword, 'limit': limit_per_request,
                'fields': 'title,authors,year,abstract,venue,citationCount,externalIds,url'
            }

            if year_from:
                params['year'] = f"{year_from}-"

            max_retries = 3
            for attempt in range(max_retries):
                try:
                    time.sleep(1)
                    response = requests.get(search_url, params=params, headers=self.headers, timeout=15)
                    if response.status_code == 429:
                        wait_time = 5 * (attempt + 1)
                        st.warning(f"Rate limitæ¤œå‡ºã€‚{wait_time}ç§’å¾…æ©Ÿä¸­...")
                        time.sleep(wait_time)
                        continue
                    response.raise_for_status()
                    data = response.json()
                    break
                except requests.exceptions.HTTPError as e:
                    if attempt == max_retries - 1:
                        raise

            for paper_data in data.get('data', []):
                try:
                    authors = [author['name'] for author in paper_data.get('authors', [])]
                    year = paper_data.get('year', 'N/A')
                    external_ids = paper_data.get('externalIds', {})
                    paper_id = paper_data.get('paperId', '')
                    url = f"https://www.semanticscholar.org/paper/{paper_id}"
                    if external_ids.get('DOI'):
                        url = f"https://doi.org/{external_ids['DOI']}"

                    paper_info = {
                        'title': paper_data.get('title', 'N/A'), 'authors': authors,
                        'year': str(year) if year else 'N/A',
                        'abstract': paper_data.get('abstract') or 'N/A',
                        'venue': paper_data.get('venue') or 'N/A', 'url': url,
                        'citations': paper_data.get('citationCount', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword, 'source': 'Semantic Scholar'
                    }

                    papers.append(paper_info)

                except:
                    continue

            return papers

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                st.error("Semantic Scholar API Rate limitã«é”ã—ã¾ã—ãŸã€‚æ•°åˆ†å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                st.info("ğŸ’¡ ä»£ã‚ã‚Šã«PubMedã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
            else:
                st.error(f"Semantic Scholar API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        return self.search_papers(keyword, max_results, year_from=current_year)


# ==================== Google Scholar Crawler ====================
class ScholarCrawler:
    """Google Scholarã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self):
        self.results = []
        try:
            from scholarly import scholarly, ProxyGenerator
            self.scholarly = scholarly
            try:
                pg = ProxyGenerator()
                pg.FreeProxies()
                scholarly.use_proxy(pg)
            except:
                pass
        except ImportError:
            st.warning("scholarly ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            self.scholarly = None

    def search_papers(self, keyword: str, max_results: int = 20, year_from: Optional[int] = None) -> List[Dict]:
        if not self.scholarly:
            st.error("Google Scholaræ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []

        papers = []
        try:
            search_query = keyword
            if year_from:
                search_query = f"{keyword} after:{year_from}"

            search_results = self.scholarly.search_pubs(search_query)
            count = 0
            for result in search_results:
                if count >= max_results:
                    break

                try:
                    paper_info = {
                        'title': result.get('bib', {}).get('title', 'N/A'),
                        'authors': result.get('bib', {}).get('author', ['N/A']),
                        'year': result.get('bib', {}).get('pub_year', 'N/A'),
                        'abstract': result.get('bib', {}).get('abstract', 'N/A'),
                        'venue': result.get('bib', {}).get('venue', 'N/A'),
                        'url': result.get('pub_url', result.get('eprint_url', 'N/A')),
                        'citations': result.get('num_citations', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword, 'source': 'Google Scholar'
                    }

                    papers.append(paper_info)
                    count += 1
                    time.sleep(2)

                except:
                    continue

            return papers

        except Exception as e:
            st.error(f"Google Scholar ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ’¡ Google ScholarãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚Semantic Scholarã¾ãŸã¯PubMedã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        from_date = datetime.now() - timedelta(days=days)
        year_from = from_date.year
        return self.search_papers(keyword, max_results, year_from)


# ==================== ãƒ†ã‚­ã‚¹ãƒˆè§£æ ====================
def extract_keywords(text: str, min_length: int = 4, top_n: int = 50) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    words = re.findall(r'\b[a-zA-Z]{' + str(min_length) + r',}\b', text.lower())
    stop_words = {
        'this', 'that', 'with', 'from', 'were', 'been', 'have', 'has', 'had',
        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
        'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all',
        'was', 'said', 'them', 'than', 'find', 'also', 'made',
        'when', 'what', 'which', 'their', 'these', 'those', 'such', 'into',
        'through', 'during', 'before', 'after', 'about', 'between', 'under'
    }
    filtered_words = [w for w in words if w not in stop_words]
    word_counts = Counter(filtered_words)
    return [word for word, _ in word_counts.most_common(top_n)]


def build_cooccurrence_network(papers: List[Dict], top_keywords: int = 30, window_size: int = 10):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    all_text = " ".join([f"{p['title']} {p['abstract']}" for p in papers if p['abstract'] != 'N/A'])
    keywords = extract_keywords(all_text, min_length=5, top_n=top_keywords)
    cooccurrence = Counter()

    for paper in papers:
        text = f"{paper['title']} {paper['abstract']}"
        words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
        for i, word1 in enumerate(words):
            if word1 not in keywords:
                continue
            for j in range(i + 1, min(i + window_size, len(words))):
                word2 = words[j]
                if word2 in keywords and word1 != word2:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] += 1
    return keywords, cooccurrence


# ==================== Gemini AIè¦ç´„ ====================
def summarize_papers_with_gemini(papers: List[Dict], api_key: str, search_keyword: str) -> str:
    """Gemini APIã‚’ä½¿ã£ã¦è«–æ–‡å…¨ä½“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¨è€ƒå¯Ÿã‚’ç”Ÿæˆ"""
    try:
        import google.generativeai as genai

        # APIè¨­å®š
        genai.configure(api_key=api_key)
        # gemini-1.5-flashã¯å®‰å®šç‰ˆã§ç„¡æ–™æ ãŒå¤§ãã„
        model = genai.GenerativeModel('gemini-1.5-flash')

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        papers_text = ""
        for i, paper in enumerate(papers[:20], 1):  # æœ€å¤§20ä»¶ã¾ã§
            abstract = paper.get('abstract', 'N/A')
            if abstract == 'N/A':
                abstract = "Abstract not available"
            papers_text += f"\n[Paper {i}]\nTitle: {paper['title']}\nYear: {paper['year']}\nAbstract: {abstract[:500]}...\n"

        prompt = f"""
ã‚ãªãŸã¯ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ã€Œ{search_keyword}ã€ã«é–¢ã™ã‚‹ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã¨è€ƒå¯Ÿã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã€‘
{papers_text}

ã€åˆ†æå†…å®¹ã€‘
1. **ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¦‚è¦**: ã“ã®åˆ†é‡ã§ç¾åœ¨æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒã‚„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
2. **æ™‚ç³»åˆ—çš„ãªå¤‰åŒ–**: å¹´ä»£ã«ã‚ˆã‚‹ç ”ç©¶ã®å¤‰é·ã‚„æ–°ã—ã„å‹•å‘
3. **ä¸»è¦ãªç ”ç©¶æ–¹å‘æ€§**: ã©ã®ã‚ˆã†ãªç ”ç©¶èª²é¡Œã‚„å¿œç”¨åˆ†é‡ãŒä¸»æµã‹
4. **ä»Šå¾Œã®å±•æœ›**: ã“ã®åˆ†é‡ã®ä»Šå¾Œã®ç™ºå±•å¯èƒ½æ€§ã‚„æ³¨ç›®ã™ã¹ããƒã‚¤ãƒ³ãƒˆ

ã€å‡ºåŠ›å½¢å¼ã€‘
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¦‹å‡ºã—ä»˜ãã§æ§‹é€ åŒ–
- å…·ä½“çš„ãªè«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¼•ç”¨ã—ãªãŒã‚‰èª¬æ˜
- å°‚é–€çš„ã‹ã¤åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã§è¨˜è¿°
- åˆè¨ˆ800-1200æ–‡å­—ç¨‹åº¦
"""

        # APIå‘¼ã³å‡ºã—
        response = model.generate_content(prompt)
        return response.text

    except ImportError:
        return "âŒ ã‚¨ãƒ©ãƒ¼: google-generativeai ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n\n`pip install google-generativeai` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__

        # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
        full_error = f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_type}\nã‚¨ãƒ©ãƒ¼å†…å®¹: {error_msg}"

        if "API_KEY_INVALID" in error_msg or "invalid" in error_msg.lower() and "key" in error_msg.lower():
            return f"âŒ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ã€‚æ­£ã—ã„Gemini APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n\n{full_error}"
        elif "quota" in error_msg.lower() or "RESOURCE_EXHAUSTED" in error_msg:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚\n\n{full_error}"
        elif "404" in error_msg or "not found" in error_msg.lower() or "NOT_FOUND" in error_msg:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nç¾åœ¨ä½¿ç”¨ä¸­: gemini-1.5-flash\nä»£æ›¿ãƒ¢ãƒ‡ãƒ«: gemini-1.5-pro, gemini-1.5-flash-8b\n\n{full_error}"
        elif "PERMISSION_DENIED" in error_msg or "permission" in error_msg.lower():
            return f"âŒ ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ã®æ¨©é™ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æ–°ã—ã„APIã‚­ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n{full_error}"
        elif "blocked" in error_msg.lower() or "SAFETY" in error_msg:
            return f"âŒ ã‚¨ãƒ©ãƒ¼: å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚\n\n{full_error}"
        else:
            return f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ\n\n{full_error}\n\nğŸ’¡ å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã¯ã€APIã‚­ãƒ¼ã‚’å†ç¢ºèªã™ã‚‹ã‹ã€åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆgemini-1.5-proï¼‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ ====================
def main():
    st.title("ğŸ“š Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("é«˜åº¦ãªè«–æ–‡åˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰è§£æãƒ»AIè¦ç´„ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        st.markdown("### ğŸ¤– Gemini APIè¨­å®š")
        api_key_input = st.text_input(
            "Gemini APIã‚­ãƒ¼",
            type="password",
            value=st.session_state.gemini_api_key,
            placeholder="AIã‚­ãƒ¼ã‚’å…¥åŠ›ï¼ˆAIè¦ç´„æ©Ÿèƒ½ç”¨ï¼‰"
        )
        if api_key_input:
            st.session_state.gemini_api_key = api_key_input

        st.markdown("[APIã‚­ãƒ¼å–å¾—æ–¹æ³•](https://aistudio.google.com/app/apikey)")
        st.markdown("---")

        st.markdown("### ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¯”è¼ƒ")
        st.markdown("""
        **PubMed**
        - åŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦ç‰¹åŒ–
        - å…¬å¼APIãƒ»å®‰å®š
        - å¼•ç”¨æ•°ãªã—

        **Semantic Scholar** â­
        - å…¨åˆ†é‡å¯¾å¿œ
        - å¼•ç”¨æ•°ã‚ã‚Š
        - ç„¡æ–™ãƒ»å®‰å®š

        **Google Scholar**
        - æœ€å¤§ã®DB
        - ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚„ã™ã„
        """)

    # ã‚¿ãƒ–
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ“š è«–æ–‡æ¤œç´¢", "ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰", "ğŸ“Š çµ±è¨ˆåˆ†æ", "ğŸ¤– AIè¦ç´„",
        "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿"
    ])

    # ã‚¿ãƒ–1: è«–æ–‡æ¤œç´¢
    with tab1:
        st.header("è«–æ–‡æ¤œç´¢")

        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            ["PubMedï¼ˆåŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦ï¼‰", "Semantic Scholarï¼ˆå…¨åˆ†é‡ãƒ»å¼•ç”¨æ•°ã‚ã‚Šï¼‰", "Google Scholarï¼ˆãƒ–ãƒ­ãƒƒã‚¯æ³¨æ„ï¼‰"]
        )

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹: mass spectrometry proteomics")
        with col2:
            max_results = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=100, value=10)

        year_filter = st.checkbox("å¹´ã§çµã‚Šè¾¼ã¿")
        year_from = None
        if year_filter:
            year_from = st.slider("æ¤œç´¢é–‹å§‹å¹´", 2000, datetime.now().year, 2020)

        if st.button("ğŸ” è«–æ–‡ã‚’æ¤œç´¢", type="primary"):
            if query:
                with st.spinner(f"{data_source}ã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                    try:
                        if "PubMed" in data_source:
                            crawler = PubMedCrawler()
                        elif "Semantic Scholar" in data_source:
                            crawler = SemanticScholarCrawler()
                        else:
                            crawler = ScholarCrawler()

                        papers = crawler.search_papers(query, max_results, year_from)

                        if papers:
                            st.session_state.papers = papers
                            st.session_state.search_keyword = query  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¿å­˜
                            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")
                            st.info("ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ã¯ã€ŒğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã€ã‚¿ãƒ–ã§ã„ã¤ã§ã‚‚ç¢ºèªãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™")
                        else:
                            st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

        # æ¤œç´¢çµæœã®è¡¨ç¤ºï¼ˆæ¤œç´¢ãƒœã‚¿ãƒ³ã®å¤–ã«é…ç½®ï¼‰
        if st.session_state.papers:
            st.markdown("---")
            st.markdown(f"### ğŸ“„ æ¤œç´¢çµæœï¼ˆå…¨{len(st.session_state.papers)}ä»¶ï¼‰")

            # ã‚½ãƒ¼ãƒˆã¨è¡¨ç¤ºä»¶æ•°ã®é¸æŠ
            col1, col2 = st.columns([1, 1])

            with col1:
                sort_option = st.selectbox(
                    "ä¸¦ã³æ›¿ãˆ",
                    options=[
                        "é–¢é€£æ€§é †ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰",
                        "æ–°ã—ã„é †ï¼ˆå¹´é™é †ï¼‰",
                        "å¤ã„é †ï¼ˆå¹´æ˜‡é †ï¼‰",
                        "å¼•ç”¨æ•°é †ï¼ˆå¤šã„é †ï¼‰",
                        "è‘—è€…åé †ï¼ˆA-Zï¼‰",
                        "ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åé †ï¼ˆA-Zï¼‰"
                    ],
                    index=0,
                    key="sort_option_select"
                )

            with col2:
                display_options = [10, 20, 50, 100]
                if len(st.session_state.papers) > 100:
                    display_options.append("å…¨ä»¶è¡¨ç¤º")
                else:
                    display_options.append(f"å…¨{len(st.session_state.papers)}ä»¶")

                display_count = st.selectbox(
                    "è¡¨ç¤ºä»¶æ•°",
                    options=display_options,
                    index=0,
                    key="display_count_select"
                )

            # ã‚½ãƒ¼ãƒˆå‡¦ç†
            sorted_papers = st.session_state.papers.copy()

            if sort_option == "æ–°ã—ã„é †ï¼ˆå¹´é™é †ï¼‰":
                sorted_papers = sorted(
                    sorted_papers,
                    key=lambda x: int(x['year']) if x['year'] != 'N/A' and str(x['year']).isdigit() else 0,
                    reverse=True
                )
            elif sort_option == "å¤ã„é †ï¼ˆå¹´æ˜‡é †ï¼‰":
                sorted_papers = sorted(
                    sorted_papers,
                    key=lambda x: int(x['year']) if x['year'] != 'N/A' and str(x['year']).isdigit() else 9999,
                    reverse=False
                )
            elif sort_option == "å¼•ç”¨æ•°é †ï¼ˆå¤šã„é †ï¼‰":
                sorted_papers = sorted(
                    sorted_papers,
                    key=lambda x: x.get('citations', 0),
                    reverse=True
                )
            elif sort_option == "è‘—è€…åé †ï¼ˆA-Zï¼‰":
                sorted_papers = sorted(
                    sorted_papers,
                    key=lambda x: (x['authors'][0] if isinstance(x['authors'], list) and len(x['authors']) > 0 else x['authors']) if x['authors'] else 'zzz'
                )
            elif sort_option == "ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åé †ï¼ˆA-Zï¼‰":
                sorted_papers = sorted(
                    sorted_papers,
                    key=lambda x: x.get('venue', 'zzz') if x.get('venue') != 'N/A' else 'zzz'
                )
            # é–¢é€£æ€§é †ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„

            if isinstance(display_count, str):  # "å…¨ä»¶è¡¨ç¤º" or "å…¨Xä»¶"
                display_count = len(sorted_papers)

            papers_to_display = sorted_papers[:display_count]

            for i, paper in enumerate(papers_to_display, 1):
                with st.expander(f"ğŸ“„ {i}. {paper['title'][:80]}..."):
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        authors_str = ', '.join(paper['authors'][:3]) if isinstance(paper['authors'], list) else paper['authors']
                        st.markdown(f"**è‘—è€…**: {authors_str}")
                        st.markdown(f"**å¹´**: {paper['year']}")
                        st.markdown(f"**æ²è¼‰**: {paper.get('venue', 'N/A')}")
                        st.markdown(f"**URL**: [{paper['url']}]({paper['url']})")
                    with col2:
                        if paper.get('citations', 0) > 0:
                            st.metric("å¼•ç”¨æ•°", paper['citations'])

                    if paper.get('abstract') and paper['abstract'] != 'N/A':
                        st.markdown(f"**è¦æ—¨**: {paper['abstract'][:400]}...")

    # ã‚¿ãƒ–2: ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰
    with tab2:
        st.header("ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")

        if st.session_state.papers:
            st.subheader("Year-wise Publication Trend")
            years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A' and str(p['year']).isdigit()]

            if years:
                year_counts = Counter(years)
                year_df = pd.DataFrame(list(year_counts.items()), columns=['Year', 'Count']).sort_values('Year')

                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(year_df['Year'], year_df['Count'], marker='o', linewidth=2, markersize=8)
                ax.set_xlabel('Year', fontsize=12)
                ax.set_ylabel('Number of Papers', fontsize=12)
                ax.set_title('Publication Trend by Year', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Papers", len(years))
                with col2:
                    peak_year = year_counts.most_common(1)[0][0]
                    st.metric("Peak Year", peak_year)
                with col3:
                    avg_per_year = len(years) / len(year_counts) if year_counts else 0
                    st.metric("Avg/Year", f"{avg_per_year:.1f}")
            else:
                st.warning("å¹´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–3: çµ±è¨ˆçš„å…¨ä½“å‚¾å‘åˆ†æ
    with tab3:
        st.header("ğŸ“Š çµ±è¨ˆåˆ†æ")
        st.markdown("æ¤œç´¢ã—ãŸè«–æ–‡å…¨ä½“ã®ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’çµ±è¨ˆçš„ã«åˆ†æã—ã¾ã™ï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰")

        if st.session_state.papers:
            if st.button("ğŸ“Š çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ", type="primary"):
                papers_to_analyze = st.session_state.papers

                with st.spinner("åˆ†æä¸­..."):
                    # 1. åŸºæœ¬çµ±è¨ˆ
                    st.markdown("---")
                    st.markdown("### ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ")
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("ç·è«–æ–‡æ•°", len(papers_to_analyze))

                    with col2:
                        years = [p['year'] for p in papers_to_analyze if p['year'] != 'N/A' and str(p['year']).isdigit()]
                        if years:
                            year_range = f"{min(years)}-{max(years)}"
                            st.metric("å¯¾è±¡å¹´ç¯„å›²", year_range)

                    with col3:
                        total_citations = sum([p.get('citations', 0) for p in papers_to_analyze])
                        st.metric("ç·å¼•ç”¨æ•°", total_citations)

                    with col4:
                        avg_citations = total_citations / len(papers_to_analyze) if papers_to_analyze else 0
                        st.metric("å¹³å‡å¼•ç”¨æ•°", f"{avg_citations:.1f}")

                    # 2. é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
                    st.markdown("---")
                    st.markdown("### ğŸ”‘ é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Top 20")
                    all_text = " ".join([f"{p['title']} {p['abstract']}" for p in papers_to_analyze if p['abstract'] != 'N/A'])
                    keywords = extract_keywords(all_text, min_length=5, top_n=20)

                    if keywords:
                        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’è¨ˆç®—
                        keyword_counts = Counter()
                        for paper in papers_to_analyze:
                            text = f"{paper['title']} {paper['abstract']}".lower()
                            for kw in keywords:
                                keyword_counts[kw] += text.count(kw)

                        # æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ç¤º
                        kw_df = pd.DataFrame(list(keyword_counts.most_common(20)), columns=['Keyword', 'Count'])
                        fig, ax = plt.subplots(figsize=(12, 6))
                        ax.barh(kw_df['Keyword'], kw_df['Count'], color='skyblue')
                        ax.set_xlabel('Frequency', fontsize=12)
                        ax.set_ylabel('Keywords', fontsize=12)
                        ax.set_title('Top 20 Keywords', fontsize=14, fontweight='bold')
                        ax.invert_yaxis()
                        st.pyplot(fig)

                    # 3. å¹´ä»£åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
                    st.markdown("---")
                    st.markdown("### ğŸ“… å¹´ä»£åˆ¥ã®ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰")
                    if years:
                        year_keywords = {}
                        for year in sorted(set(years)):
                            year_papers = [p for p in papers_to_analyze if str(p['year']) == str(year)]
                            year_text = " ".join([f"{p['title']} {p['abstract']}" for p in year_papers if p['abstract'] != 'N/A'])
                            year_kws = extract_keywords(year_text, min_length=5, top_n=5)
                            year_keywords[year] = year_kws

                        for year in sorted(year_keywords.keys()):
                            st.markdown(f"**{year}å¹´**: {', '.join(year_keywords[year][:5])}")

                    # 4. ä¸»è¦è‘—è€…åˆ†æ
                    st.markdown("---")
                    st.markdown("### ğŸ‘¥ ä¸»è¦è‘—è€… Top 10")
                    all_authors = []
                    for paper in papers_to_analyze:
                        authors = paper['authors']
                        if isinstance(authors, list):
                            all_authors.extend(authors)
                        else:
                            all_authors.append(authors)

                    author_counts = Counter(all_authors)
                    top_authors = author_counts.most_common(10)

                    if top_authors:
                        author_df = pd.DataFrame(top_authors, columns=['Author', 'Papers'])
                        st.dataframe(author_df, use_container_width=True)

                    # 5. æ²è¼‰ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«åˆ†æ
                    st.markdown("---")
                    st.markdown("### ğŸ“š ä¸»è¦æ²è¼‰ã‚¸ãƒ£ãƒ¼ãƒŠãƒ« Top 10")
                    venues = [p['venue'] for p in papers_to_analyze if p.get('venue') and p['venue'] != 'N/A']
                    venue_counts = Counter(venues)
                    top_venues = venue_counts.most_common(10)

                    if top_venues:
                        venue_df = pd.DataFrame(top_venues, columns=['Journal', 'Papers'])
                        st.dataframe(venue_df, use_container_width=True)

                    # 6. å¼•ç”¨æ•°åˆ†å¸ƒ
                    st.markdown("---")
                    st.markdown("### ğŸ“Š å¼•ç”¨æ•°åˆ†å¸ƒ")
                    citations = [p.get('citations', 0) for p in papers_to_analyze if p.get('citations', 0) > 0]

                    if citations:
                        fig, ax = plt.subplots(figsize=(12, 5))
                        ax.hist(citations, bins=20, color='lightcoral', edgecolor='black', alpha=0.7)
                        ax.set_xlabel('Citations', fontsize=12)
                        ax.set_ylabel('Number of Papers', fontsize=12)
                        ax.set_title('Citation Distribution', fontsize=14, fontweight='bold')
                        ax.grid(True, alpha=0.3)
                        st.pyplot(fig)

                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æœ€å¤šå¼•ç”¨æ•°", max(citations))
                        with col2:
                            st.metric("ä¸­å¤®å€¤", int(pd.Series(citations).median()))
                        with col3:
                            st.metric("å¹³å‡å€¤", f"{pd.Series(citations).mean():.1f}")

                    st.success("âœ… çµ±è¨ˆåˆ†æå®Œäº†ï¼")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–4: AIè¦ç´„
    with tab4:
        st.header("ğŸ¤– AIè¦ç´„ï¼ˆGeminiï¼‰")
        st.markdown("""
        ### ğŸ“– AIè¦ç´„ã¨ã¯ï¼Ÿ
        Google Gemini AIã‚’ä½¿ã£ã¦ã€æ¤œç´¢ã—ãŸè«–æ–‡ã®**ã‚¿ã‚¤ãƒˆãƒ«**ã¨**è¦æ—¨ï¼ˆAbstractï¼‰**ã‹ã‚‰ã€
        ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã¨è€ƒå¯Ÿã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚

        **åˆ†æå†…å®¹:**
        - ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¦‚è¦
        - æ™‚ç³»åˆ—çš„ãªå¤‰åŒ–
        - ä¸»è¦ãªç ”ç©¶æ–¹å‘æ€§
        - ä»Šå¾Œã®å±•æœ›

        **æ³¨æ„:** Gemini APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šï¼‰
        """)
        st.markdown("---")

        if st.session_state.papers:
            if not st.session_state.gemini_api_key:
                st.warning("âš ï¸ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            else:
                st.info(f"ğŸ“Š ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™ï¼ˆæœ€å¤§20ä»¶ã¾ã§åˆ†æï¼‰")

                # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å–å¾—ï¼ˆsession_stateã«ä¿å­˜ã™ã‚‹ï¼‰
                if 'search_keyword' not in st.session_state:
                    st.session_state.search_keyword = st.session_state.papers[0].get('keyword', 'Unknown') if st.session_state.papers else 'Unknown'

                if st.button("ğŸ¤– AIè¦ç´„ã‚’ç”Ÿæˆ", type="primary"):
                    with st.spinner("Gemini AIãŒåˆ†æä¸­...ï¼ˆ30ç§’ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ï¼‰"):
                        summary = summarize_papers_with_gemini(
                            st.session_state.papers,
                            st.session_state.gemini_api_key,
                            st.session_state.search_keyword
                        )

                        st.markdown("---")
                        st.markdown("### ğŸ“ AIç”Ÿæˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
                        st.markdown(summary)
                        st.success("âœ… AIè¦ç´„å®Œäº†ï¼")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–5: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    with tab5:
        st.header("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ")
        st.markdown("""
        ### ğŸ“– ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã¨ã¯ï¼Ÿ
        æ¤œç´¢ã—ãŸè«–æ–‡ã®**ã‚¿ã‚¤ãƒˆãƒ«**ã¨**è¦æ—¨ï¼ˆAbstractï¼‰**ã‹ã‚‰é »å‡ºå˜èªã‚’æŠ½å‡ºã—ã€
        å‡ºç¾é »åº¦ã«å¿œã˜ã¦æ–‡å­—ã‚µã‚¤ã‚ºã‚’å¤‰ãˆã¦è¦–è¦šåŒ–ã—ã¾ã™ã€‚

        **æ´»ç”¨æ–¹æ³•:**
        - ç ”ç©¶åˆ†é‡ã§é »ç¹ã«ä½¿ã‚ã‚Œã‚‹å°‚é–€ç”¨èªã‚’ä¸€ç›®ã§æŠŠæ¡
        - ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã®ä¸­å¿ƒçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç™ºè¦‹
        - ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è³‡æ–™ã‚„ãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆã«æ´»ç”¨
        """)
        st.markdown("---")

        if st.session_state.papers:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™")
            with col2:
                max_words = st.slider("æœ€å¤§å˜èªæ•°", 30, 200, 100)

            if st.button("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ"):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    text = " ".join([f"{p['title']} {p['abstract']}" for p in st.session_state.papers if p['abstract'] != 'N/A'])
                    if text:
                        wordcloud = WordCloud(width=1200, height=600, background_color='white', colormap='viridis', max_words=max_words).generate(text)
                        fig, ax = plt.subplots(figsize=(15, 7))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        st.success("âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–6: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    with tab6:
        st.header("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ")
        st.markdown("""
        ### ğŸ“– å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã¯ï¼Ÿ
        æ¤œç´¢ã—ãŸè«–æ–‡ã®**ã‚¿ã‚¤ãƒˆãƒ«**ã¨**è¦æ—¨ï¼ˆAbstractï¼‰**ã‹ã‚‰ã€
        **åŒã˜æ–‡è„ˆã§ä¸€ç·’ã«å‡ºç¾ã™ã‚‹å˜èªï¼ˆå…±èµ·é–¢ä¿‚ï¼‰**ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã¨ã—ã¦å¯è¦–åŒ–ã—ã¾ã™ã€‚

        **èª­ã¿æ–¹:**
        - **ãƒãƒ¼ãƒ‰ï¼ˆå††ï¼‰**: é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€‚å††ãŒå¤§ãã„ã»ã©ä»–ã®å˜èªã¨ã®é–¢é€£æ€§ãŒé«˜ã„
        - **ã‚¨ãƒƒã‚¸ï¼ˆç·šï¼‰**: å˜èªé–“ã®å…±èµ·é–¢ä¿‚ã€‚ç·šãŒå¤ªã„ã»ã©ä¸€ç·’ã«å‡ºç¾ã™ã‚‹å›æ•°ãŒå¤šã„
        - **ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼**: å¯†ã«ç¹‹ãŒã£ã¦ã„ã‚‹å˜èªç¾¤ã¯ã€é–¢é€£ã™ã‚‹ç ”ç©¶ãƒ†ãƒ¼ãƒã‚’ç¤ºã™

        **æ´»ç”¨æ–¹æ³•:**
        - ç ”ç©¶åˆ†é‡å†…ã®æ¦‚å¿µåŒå£«ã®é–¢é€£æ€§ã‚’æŠŠæ¡
        - æ–°ã—ã„ç ”ç©¶ã‚¢ã‚¤ãƒ‡ã‚¢ã®ç™ºè¦‹ï¼ˆæ„å¤–ãªå˜èªã®çµ„ã¿åˆã‚ã›ï¼‰
        - ç ”ç©¶é ˜åŸŸã®ãƒãƒƒãƒ—ä½œæˆ
        - æ–‡çŒ®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®æ§‹é€ åŒ–

        **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¬æ˜:**
        - **è¡¨ç¤ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å«ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ•°
        - **å…±èµ·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦**: ä½•å˜èªé›¢ã‚Œã¦ã„ã¦ã‚‚ã€Œå…±èµ·ã€ã¨ã¿ãªã™ã‹ï¼ˆå¤§ãã„ã»ã©åºƒç¯„å›²ï¼‰
        - **æœ€å°å…±èµ·å›æ•°**: ä½•å›ä»¥ä¸Šä¸€ç·’ã«å‡ºç¾ã—ãŸå˜èªã‚’ç·šã§çµã¶ã‹ï¼ˆå¤§ãã„ã»ã©å¼·ã„é–¢ä¿‚ã®ã¿è¡¨ç¤ºï¼‰
        """)
        st.markdown("---")

        if st.session_state.papers:
            col1, col2, col3 = st.columns(3)
            with col1:
                top_keywords = st.slider("è¡¨ç¤ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 10, 50, 30)
            with col2:
                window_size = st.slider("å…±èµ·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦", 5, 20, 10)
            with col3:
                min_cooccurrence = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 10, 2)

            if st.button("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆ"):
                with st.spinner("è§£æä¸­..."):
                    keywords, cooccurrence = build_cooccurrence_network(st.session_state.papers, top_keywords, window_size)
                    G = nx.Graph()
                    for (word1, word2), count in cooccurrence.items():
                        if count >= min_cooccurrence:
                            G.add_edge(word1, word2, weight=count)

                    if len(G.nodes()) > 0:
                        pos = nx.spring_layout(G, k=0.5, iterations=50)
                        fig, ax = plt.subplots(figsize=(16, 12))
                        node_sizes = [G.degree(node) * 300 for node in G.nodes()]
                        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.7, ax=ax)
                        edges = G.edges()
                        weights = [G[u][v]['weight'] for u, v in edges]
                        max_weight = max(weights) if weights else 1
                        nx.draw_networkx_edges(G, pos, width=[w / max_weight * 5 for w in weights], alpha=0.3, ax=ax)
                        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', ax=ax)
                        ax.axis('off')
                        ax.set_title(f"Co-occurrence Network (Nodes: {len(G.nodes())}, Edges: {len(G.edges())})", fontsize=16)
                        st.pyplot(fig)

                        st.subheader("Network Statistics")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Nodes", len(G.nodes()))
                        with col2:
                            st.metric("Edges", len(G.edges()))
                        with col3:
                            if len(G.nodes()) > 0:
                                avg_degree = sum(dict(G.degree()).values()) / len(G.nodes())
                                st.metric("Avg Degree", f"{avg_degree:.2f}")
                        st.success("âœ… å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–7: ä¿å­˜ãƒ‡ãƒ¼ã‚¿
    with tab7:
        st.header("ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿")
        if st.session_state.papers:
            st.subheader("ğŸ† å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆTop 10ï¼‰")
            papers_with_citations = [p for p in st.session_state.papers if p.get('citations', 0) > 0]
            if papers_with_citations:
                sorted_papers = sorted(papers_with_citations, key=lambda x: x.get('citations', 0), reverse=True)[:10]
                for i, paper in enumerate(sorted_papers, 1):
                    col1, col2 = st.columns([5, 1])
                    with col1:
                        st.markdown(f"**{i}. {paper['title'][:70]}...**")
                        authors_str = ', '.join(paper['authors'][:2]) if isinstance(paper['authors'], list) else paper['authors']
                        st.caption(f"{authors_str} ({paper['year']})")
                    with col2:
                        st.metric("Citations", paper['citations'])
            else:
                st.info("å¼•ç”¨æ•°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            st.markdown("---")
            st.subheader("ğŸ“š å–å¾—æ¸ˆã¿è«–æ–‡")
            df_data = []
            for p in st.session_state.papers:
                authors_list = p['authors']
                if isinstance(authors_list, list):
                    authors_str = ', '.join(authors_list[:2])
                    if len(authors_list) > 2:
                        authors_str += '...'
                else:
                    authors_str = authors_list[:50]
                df_data.append({
                    'Title': p['title'][:60] + '...' if len(p['title']) > 60 else p['title'],
                    'Authors': authors_str, 'Year': p['year'],
                    'Source': p.get('source', 'N/A'), 'Citations': p.get('citations', 0)
                })
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

            csv = pd.DataFrame(st.session_state.papers).to_csv(index=False, encoding='utf-8-sig')
            st.download_button(label="ğŸ“¥ CSV Download", data=csv, file_name=f"papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")

            st.subheader("ğŸ“Š Statistics")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Papers", len(st.session_state.papers))
            with col2:
                years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A']
                if years:
                    year_counts = Counter(years)
                    most_common_year = year_counts.most_common(1)[0][0]
                    st.metric("Most Common Year", most_common_year)
            with col3:
                if years:
                    st.metric("Latest Year", max(years))
            with col4:
                total_citations = sum([p.get('citations', 0) for p in st.session_state.papers])
                st.metric("Total Citations", total_citations)
        else:
            st.info("ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
