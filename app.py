"""
Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé«˜åº¦åˆ†æç‰ˆï¼‰
- ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
- AIä¸€æ‹¬è¦ç´„ï¼ˆGemini APIï¼‰
- å¼•ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
"""
import streamlit as st
import sys
import os
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import time
import requests
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET
import re
from itertools import combinations
import networkx as nx
import json

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="è«–æ–‡æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”¬",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'papers' not in st.session_state:
    st.session_state.papers = []
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ""


# ==================== PubMed Crawler ====================
class PubMedCrawler:
    """PubMed APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self, email: str = "user@example.com"):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = email

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'relevance'
            }

            if year_from:
                current_year = datetime.now().year
                search_params['datetype'] = 'pdat'
                search_params['mindate'] = f"{year_from}/01/01"
                search_params['maxdate'] = f"{current_year}/12/31"

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            batch_size = 5
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                ids_str = ','.join(batch_ids)

                fetch_params = {
                    'db': 'pubmed',
                    'id': ids_str,
                    'retmode': 'xml',
                    'email': self.email
                }

                fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
                fetch_response.raise_for_status()
                root = ET.fromstring(fetch_response.content)

                for article in root.findall('.//PubmedArticle'):
                    try:
                        paper_info = self._extract_paper_info(article, keyword)
                        papers.append(paper_info)
                    except:
                        continue
                time.sleep(0.5)

            return papers

        except Exception as e:
            st.error(f"PubMed API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def _extract_paper_info(self, article_xml, keyword: str) -> Dict:
        title_elem = article_xml.find('.//ArticleTitle')
        title = title_elem.text if title_elem is not None else 'N/A'

        authors = []
        for author in article_xml.findall('.//Author'):
            lastname = author.find('LastName')
            forename = author.find('ForeName')
            if lastname is not None:
                name = lastname.text
                if forename is not None:
                    name = f"{forename.text} {name}"
                authors.append(name)

        year_elem = article_xml.find('.//PubDate/Year')
        year = year_elem.text if year_elem is not None else 'N/A'

        abstract_texts = []
        for abstract in article_xml.findall('.//AbstractText'):
            if abstract.text:
                abstract_texts.append(abstract.text)
        abstract = ' '.join(abstract_texts) if abstract_texts else 'N/A'

        journal_elem = article_xml.find('.//Journal/Title')
        venue = journal_elem.text if journal_elem is not None else 'N/A'

        pmid_elem = article_xml.find('.//PMID')
        pmid = pmid_elem.text if pmid_elem is not None else ''
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else 'N/A'

        return {
            'title': title,
            'authors': authors,
            'year': year,
            'abstract': abstract,
            'venue': venue,
            'url': url,
            'pmid': pmid,
            'citations': 0,
            'crawled_at': datetime.now().isoformat(),
            'keyword': keyword,
            'source': 'PubMed'
        }

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'date',
                'reldate': days
            }

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            ids_str = ','.join(id_list)

            fetch_params = {
                'db': 'pubmed',
                'id': ids_str,
                'retmode': 'xml',
                'email': self.email
            }

            fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.content)

            for article in root.findall('.//PubmedArticle'):
                try:
                    paper_info = self._extract_paper_info(article, keyword)
                    papers.append(paper_info)
                except:
                    continue

            return papers

        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers


# ==================== Semantic Scholar Crawler ====================
class SemanticScholarCrawler:
    """Semantic Scholar APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}/paper/search"
            
            params = {
                'query': keyword,
                'limit': min(max_results, 100),
                'fields': 'title,authors,year,abstract,venue,citationCount,externalIds,url,publicationDate'
            }

            if year_from:
                params['year'] = f"{year_from}-"

            response = requests.get(search_url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()

            for paper_data in data.get('data', []):
                try:
                    authors = [author['name'] for author in paper_data.get('authors', [])]
                    year = paper_data.get('year', 'N/A')
                    
                    external_ids = paper_data.get('externalIds', {})
                    paper_id = paper_data.get('paperId', '')
                    url = f"https://www.semanticscholar.org/paper/{paper_id}"
                    
                    if external_ids.get('DOI'):
                        url = f"https://doi.org/{external_ids['DOI']}"

                    paper_info = {
                        'title': paper_data.get('title', 'N/A'),
                        'authors': authors,
                        'year': str(year) if year else 'N/A',
                        'abstract': paper_data.get('abstract', 'N/A'),
                        'venue': paper_data.get('venue', 'N/A'),
                        'url': url,
                        'citations': paper_data.get('citationCount', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword,
                        'source': 'Semantic Scholar'
                    }

                    papers.append(paper_info)

                except Exception as e:
                    continue

            return papers

        except Exception as e:
            st.error(f"Semantic Scholar API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        return self.search_papers(keyword, max_results, year_from=current_year)


# ==================== Google Scholar Crawler ====================
class ScholarCrawler:
    """Google Scholarã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self):
        self.results = []
        try:
            from scholarly import scholarly, ProxyGenerator
            self.scholarly = scholarly
            
            try:
                pg = ProxyGenerator()
                pg.FreeProxies()
                scholarly.use_proxy(pg)
            except:
                pass
        except ImportError:
            st.warning("scholarly ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            self.scholarly = None

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        if not self.scholarly:
            st.error("Google Scholaræ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []

        papers = []
        try:
            search_query = keyword
            if year_from:
                search_query = f"{keyword} after:{year_from}"

            search_results = self.scholarly.search_pubs(search_query)

            count = 0
            for result in search_results:
                if count >= max_results:
                    break

                try:
                    paper_info = {
                        'title': result.get('bib', {}).get('title', 'N/A'),
                        'authors': result.get('bib', {}).get('author', []),
                        'year': result.get('bib', {}).get('pub_year', 'N/A'),
                        'abstract': result.get('bib', {}).get('abstract', 'N/A'),
                        'venue': result.get('bib', {}).get('venue', 'N/A'),
                        'url': result.get('pub_url', result.get('eprint_url', 'N/A')),
                        'citations': result.get('num_citations', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword,
                        'source': 'Google Scholar'
                    }

                    papers.append(paper_info)
                    count += 1
                    time.sleep(2)

                except Exception as e:
                    continue

            return papers

        except Exception as e:
            st.error(f"Google Scholar ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ’¡ Google ScholarãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚Semantic Scholarã¾ãŸã¯PubMedã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        target_date = datetime.now() - timedelta(days=days)
        year_from = target_date.year
        return self.search_papers(keyword, max_results, year_from)


# ==================== AIè¦ç´„ï¼ˆGemini APIï¼‰ ====================
def summarize_papers_with_gemini(papers: List[Dict], api_key: str, language: str = "japanese") -> Dict[str, str]:
    """Gemini APIã§è«–æ–‡ã‚’ä¸€æ‹¬è¦ç´„"""
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-pro')
        
        summaries = {}
        
        for i, paper in enumerate(papers):
            try:
                if paper['abstract'] == 'N/A':
                    continue
                
                prompt = f"""
ä»¥ä¸‹ã®è«–æ–‡ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰ã€‚
Mass Spectrometryåˆ†é‡ã®ç ”ç©¶è€…å‘ã‘ã«ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ¼ã•ãˆã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']}
è‘—è€…: {', '.join(paper['authors'][:3]) if isinstance(paper['authors'], list) else paper['authors']}
å¹´: {paper['year']}
è¦æ—¨: {paper['abstract'][:1000]}

è¦ç´„:
"""
                
                response = model.generate_content(prompt)
                summary = response.text
                summaries[paper['title']] = summary
                
                time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
                
            except Exception as e:
                summaries[paper['title']] = f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"
                continue
        
        return summaries
    
    except Exception as e:
        st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


# ==================== ãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====================
def extract_keywords(text: str, min_length: int = 4, top_n: int = 50) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    words = re.findall(r'\b[a-zA-Z]{' + str(min_length) + r',}\b', text.lower())
    
    stop_words = {
        'this', 'that', 'with', 'from', 'were', 'been', 'have', 'has', 'had',
        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
        'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can',
        'was', 'said', 'them', 'been', 'than', 'find', 'also', 'made',
        'when', 'what', 'which', 'their', 'these', 'those', 'such', 'into',
        'through', 'during', 'before', 'after', 'about', 'between', 'under'
    }
    
    filtered_words = [w for w in words if w not in stop_words]
    word_counts = Counter(filtered_words)
    return [word for word, _ in word_counts.most_common(top_n)]


def build_cooccurrence_network(papers: List[Dict], top_keywords: int = 30, window_size: int = 10):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    all_text = " ".join([
        f"{p['title']} {p['abstract']}"
        for p in papers
        if p['abstract'] != 'N/A'
    ])
    
    keywords = extract_keywords(all_text, min_length=5, top_n=top_keywords)
    cooccurrence = Counter()
    
    for paper in papers:
        text = f"{paper['title']} {paper['abstract']}"
        words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
        
        for i, word1 in enumerate(words):
            if word1 not in keywords:
                continue
            
            for j in range(i + 1, min(i + window_size, len(words))):
                word2 = words[j]
                if word2 in keywords and word1 != word2:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] += 1
    
    return keywords, cooccurrence


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª ====================
def main():
    st.title("ğŸ”¬ Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("é«˜åº¦ãªè«–æ–‡åˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰è§£æãƒ»AIè¦ç´„ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: API Keyè¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        gemini_key = st.text_input(
            "Google Gemini API Key",
            type="password",
            value=st.session_state.gemini_api_key,
            help="https://makersuite.google.com/app/apikey ã§å–å¾—ï¼ˆç„¡æ–™ï¼‰"
        )
        if gemini_key:
            st.session_state.gemini_api_key = gemini_key
            st.success("APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿")

    # ã‚¿ãƒ–
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“š è«–æ–‡æ¤œç´¢",
        "ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰",
        "ğŸ¤– AIä¸€æ‹¬è¦ç´„",
        "ğŸ“Š ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰",
        "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "ğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿"
    ])

    # ã‚¿ãƒ–1: è«–æ–‡æ¤œç´¢
    with tab1:
        st.header("è«–æ–‡æ¤œç´¢")

        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            ["PubMedï¼ˆåŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦ï¼‰", "Semantic Scholarï¼ˆå…¨åˆ†é‡ãƒ»å¼•ç”¨æ•°ã‚ã‚Šï¼‰", "Google Scholarï¼ˆãƒ–ãƒ­ãƒƒã‚¯æ³¨æ„ï¼‰"],
            help="Semantic ScholarãŒæœ€ã‚‚ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ããŠã™ã™ã‚ã§ã™"
        )

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                placeholder="ä¾‹: mass spectrometry proteomics"
            )
        with col2:
            max_results = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=50, value=10)

        col3, col4 = st.columns(2)
        with col3:
            year_from = st.number_input("æ¤œç´¢é–‹å§‹å¹´", min_value=2000, max_value=2030, value=2015)
        with col4:
            search_mode = st.selectbox("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["é€šå¸¸æ¤œç´¢", "æœ€è¿‘ã®è«–æ–‡ï¼ˆç›´è¿‘7æ—¥ï¼‰"])

        if st.button("ğŸ” æ¤œç´¢é–‹å§‹", type="primary"):
            if not query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                with st.spinner(f"{data_source}ã§è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                    try:
                        if "PubMed" in data_source:
                            crawler = PubMedCrawler()
                        elif "Semantic Scholar" in data_source:
                            crawler = SemanticScholarCrawler()
                        else:
                            crawler = ScholarCrawler()

                        if search_mode == "é€šå¸¸æ¤œç´¢":
                            papers = crawler.search_papers(query, max_results, year_from)
                        else:
                            papers = crawler.get_recent_papers(query, days=7, max_results=max_results)

                        if papers:
                            st.session_state.papers = papers
                            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆã‚½ãƒ¼ã‚¹: {data_source}ï¼‰")

                            for i, paper in enumerate(papers[:10], 1):
                                with st.expander(f"ğŸ“„ {i}. {paper['title'][:80]}..."):
                                    authors_list = paper['authors']
                                    if isinstance(authors_list, list):
                                        authors_str = ', '.join(authors_list[:3])
                                        if len(authors_list) > 3:
                                            authors_str += f" ä»–{len(authors_list) - 3}å"
                                    else:
                                        authors_str = authors_list
                                    
                                    st.markdown(f"**è‘—è€…**: {authors_str}")
                                    st.markdown(f"**ç™ºè¡¨å¹´**: {paper['year']} | **ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**: {paper.get('venue', 'N/A')}")
                                    if paper.get('citations', 0) > 0:
                                        st.markdown(f"**å¼•ç”¨æ•°**: {paper['citations']}")
                                    st.markdown(f"**URL**: [{paper['url']}]({paper['url']})")
                                    if paper['abstract'] != 'N/A':
                                        st.markdown(f"**è¦æ—¨**: {paper['abstract'][:400]}...")
                        else:
                            st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # ã‚¿ãƒ–2: ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
    with tab2:
        st.header("ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
        st.markdown("è«–æ–‡ã®æ™‚ç³»åˆ—å¤‰åŒ–ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å¯è¦–åŒ–")

        if st.session_state.papers:
            st.subheader("ğŸ“… å¹´æ¬¡è«–æ–‡æ•°æ¨ç§»")
            
            years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A' and p['year'].isdigit()]
            
            if years:
                year_counts = Counter(years)
                year_df = pd.DataFrame(
                    list(year_counts.items()),
                    columns=['å¹´', 'è«–æ–‡æ•°']
                ).sort_values('å¹´')
                
                # æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(year_df['å¹´'], year_df['è«–æ–‡æ•°'], marker='o', linewidth=2, markersize=8)
                ax.set_xlabel('å¹´', fontsize=12)
                ax.set_ylabel('è«–æ–‡æ•°', fontsize=12)
                ax.set_title('å¹´æ¬¡è«–æ–‡æ•°æ¨ç§»', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                
                # çµ±è¨ˆæƒ…å ±
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç·è«–æ–‡æ•°", len(years))
                with col2:
                    peak_year = year_counts.most_common(1)[0][0]
                    st.metric("ãƒ”ãƒ¼ã‚¯å¹´", peak_year)
                with col3:
                    avg_per_year = len(years) / len(year_counts) if year_counts else 0
                    st.metric("å¹´å¹³å‡", f"{avg_per_year:.1f}ä»¶")
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š
                st.subheader("ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
                sorted_years = sorted(year_counts.items())
                if len(sorted_years) >= 3:
                    recent_3years = sum([count for year, count in sorted_years[-3:]])
                    older_3years = sum([count for year, count in sorted_years[:3]])
                    
                    if recent_3years > older_3years * 1.5:
                        st.success("ğŸ”¥ **ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰**: è¿‘å¹´ã®ç ”ç©¶ãŒæ´»ç™ºåŒ–ã—ã¦ã„ã¾ã™")
                    elif recent_3years < older_3years * 0.7:
                        st.warning("ğŸ“‰ **æ¸›å°‘ãƒˆãƒ¬ãƒ³ãƒ‰**: ç ”ç©¶æ´»å‹•ãŒæ¸›å°‘å‚¾å‘ã§ã™")
                    else:
                        st.info("â¡ï¸ **å®‰å®šãƒˆãƒ¬ãƒ³ãƒ‰**: ç ”ç©¶æ´»å‹•ã¯å®‰å®šã—ã¦ã„ã¾ã™")
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ™‚ç³»åˆ—å¤‰åŒ–
                st.subheader("ğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾ãƒˆãƒ¬ãƒ³ãƒ‰")
                
                all_text = " ".join([
                    f"{p['title']} {p['abstract']}"
                    for p in st.session_state.papers
                    if p['abstract'] != 'N/A'
                ])
                
                top_keywords = extract_keywords(all_text, min_length=5, top_n=10)
                
                if top_keywords:
                    keyword_trends = {}
                    
                    for kw in top_keywords[:5]:  # ä¸Šä½5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿
                        yearly_count = {}
                        for paper in st.session_state.papers:
                            if paper['year'] != 'N/A' and paper['year'].isdigit():
                                text = f"{paper['title']} {paper['abstract']}".lower()
                                if kw in text:
                                    yearly_count[paper['year']] = yearly_count.get(paper['year'], 0) + 1
                        
                        keyword_trends[kw] = yearly_count
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
                    fig, ax = plt.subplots(figsize=(12, 6))
                    
                    for kw, trend in keyword_trends.items():
                        sorted_trend = sorted(trend.items())
                        years_list = [year for year, _ in sorted_trend]
                        counts_list = [count for _, count in sorted_trend]
                        ax.plot(years_list, counts_list, marker='o', label=kw, linewidth=2)
                    
                    ax.set_xlabel('å¹´', fontsize=12)
                    ax.set_ylabel('å‡ºç¾å›æ•°', fontsize=12)
                    ax.set_title('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆä¸Šä½5ä»¶ï¼‰', fontsize=14, fontweight='bold')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    st.pyplot(fig)
                
            else:
                st.warning("å¹´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–3: AIä¸€æ‹¬è¦ç´„
    with tab3:
        st.header("ğŸ¤– AIä¸€æ‹¬è¦ç´„ï¼ˆGemini APIï¼‰")
        st.markdown("å–å¾—ã—ãŸè«–æ–‡ã‚’AIãŒè‡ªå‹•ã§è¦ç´„ã—ã¾ã™")

        if st.session_state.papers:
            if not st.session_state.gemini_api_key:
                st.warning("âš ï¸ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Gemini API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
                st.markdown("[Google AI Studio](https://makersuite.google.com/app/apikey)ã§ç„¡æ–™ã§å–å¾—ã§ãã¾ã™")
            else:
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãŒã‚ã‚Šã¾ã™")
                with col2:
                    max_summarize = st.number_input("è¦ç´„ã™ã‚‹ä»¶æ•°", 1, min(20, len(st.session_state.papers)), 5)
                
                if st.button("ğŸ¤– AIè¦ç´„ã‚’é–‹å§‹", type="primary"):
                    papers_to_summarize = st.session_state.papers[:max_summarize]
                    
                    with st.spinner(f"{len(papers_to_summarize)}ä»¶ã®è«–æ–‡ã‚’è¦ç´„ä¸­..."):
                        summaries = summarize_papers_with_gemini(
                            papers_to_summarize,
                            st.session_state.gemini_api_key
                        )
                        
                        if summaries:
                            st.session_state.summaries = summaries
                            st.success(f"âœ… {len(summaries)}ä»¶ã®è«–æ–‡ã‚’è¦ç´„ã—ã¾ã—ãŸ")
                            
                            # è¦ç´„çµæœã‚’è¡¨ç¤º
                            for i, (title, summary) in enumerate(summaries.items(), 1):
                                with st.expander(f"ğŸ“ {i}. {title[:70]}..."):
                                    st.markdown(f"**è¦ç´„**:")
                                    st.markdown(summary)
                                    
                                    # å…ƒè«–æ–‡ã®æƒ…å ±ã‚‚è¡¨ç¤º
                                    original_paper = next((p for p in papers_to_summarize if p['title'] == title), None)
                                    if original_paper:
                                        st.markdown(f"**URL**: [{original_paper['url']}]({original_paper['url']})")
                        else:
                            st.error("è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–4: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    with tab4:
        st.header("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ")

        if st.session_state.papers:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™")
            with col2:
                max_words = st.slider("æœ€å¤§å˜èªæ•°", 30, 200, 100)

            if st.button("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ"):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    text = " ".join([
                        f"{p['title']} {p['abstract']}"
                        for p in st.session_state.papers
                        if p['abstract'] != 'N/A'
                    ])

                    if text:
                        wordcloud = WordCloud(
                            width=1200,
                            height=600,
                            background_color='white',
                            colormap='viridis',
                            max_words=max_words
                        ).generate(text)

                        fig, ax = plt.subplots(figsize=(15, 7))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        st.success("âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–5: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    with tab5:
        st.header("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ")

        if st.session_state.papers:
            col1, col2, col3 = st.columns(3)
            with col1:
                top_keywords = st.slider("è¡¨ç¤ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 10, 50, 30)
            with col2:
                window_size = st.slider("å…±èµ·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦", 5, 20, 10)
            with col3:
                min_cooccurrence = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 10, 2)

            if st.button("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆ"):
                with st.spinner("è§£æä¸­..."):
                    keywords, cooccurrence = build_cooccurrence_network(
                        st.session_state.papers,
                        top_keywords=top_keywords,
                        window_size=window_size
                    )

                    G = nx.Graph()
                    
                    for (word1, word2), count in cooccurrence.items():
                        if count >= min_cooccurrence:
                            G.add_edge(word1, word2, weight=count)

                    if len(G.nodes()) > 0:
                        pos = nx.spring_layout(G, k=0.5, iterations=50)
                        fig, ax = plt.subplots(figsize=(16, 12))
                        
                        node_sizes = [G.degree(node) * 300 for node in G.nodes()]
                        nx.draw_networkx_nodes(
                            G, pos,
                            node_size=node_sizes,
                            node_color='lightblue',
                            alpha=0.7,
                            ax=ax
                        )

                        edges = G.edges()
                        weights = [G[u][v]['weight'] for u, v in edges]
                        max_weight = max(weights) if weights else 1
                        
                        nx.draw_networkx_edges(
                            G, pos,
                            width=[w / max_weight * 5 for w in weights],
                            alpha=0.3,
                            ax=ax
                        )

                        nx.draw_networkx_labels(
                            G, pos,
                            font_size=10,
                            font_weight='bold',
                            ax=ax
                        )

                        ax.axis('off')
                        ax.set_title(
                            f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (ãƒãƒ¼ãƒ‰æ•°: {len(G.nodes())}, ã‚¨ãƒƒã‚¸æ•°: {len(G.edges())})",
                            fontsize=16
                        )
                        
                        st.pyplot(fig)
                        
                        st.subheader("ğŸ“Š ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ãƒãƒ¼ãƒ‰æ•°", len(G.nodes()))
                        with col2:
                            st.metric("ã‚¨ãƒƒã‚¸æ•°", len(G.edges()))
                        with col3:
                            if len(G.nodes()) > 0:
                                avg_degree = sum(dict(G.degree()).values()) / len(G.nodes())
                                st.metric("å¹³å‡æ¬¡æ•°", f"{avg_degree:.2f}")

                        st.success("âœ… å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–6: ä¿å­˜ãƒ‡ãƒ¼ã‚¿
    with tab6:
        st.header("ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿")

        if st.session_state.papers:
            # å¼•ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            st.subheader("ğŸ† å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆTop 10ï¼‰")
            
            papers_with_citations = [p for p in st.session_state.papers if p.get('citations', 0) > 0]
            
            if papers_with_citations:
                sorted_papers = sorted(papers_with_citations, key=lambda x: x.get('citations', 0), reverse=True)[:10]
                
                for i, paper in enumerate(sorted_papers, 1):
                    col1, col2 = st.columns([5, 1])
                    with col1:
                        st.markdown(f"**{i}. {paper['title'][:70]}...**")
                        authors_str = ', '.join(paper['authors'][:2]) if isinstance(paper['authors'], list) else paper['authors']
                        st.caption(f"{authors_str} ({paper['year']})")
                    with col2:
                        st.metric("å¼•ç”¨æ•°", paper['citations'])
            else:
                st.info("å¼•ç”¨æ•°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆPubMedã¯å¼•ç”¨æ•°ãªã—ï¼‰")
            
            st.markdown("---")
            
            # è«–æ–‡ãƒªã‚¹ãƒˆ
            st.subheader("ğŸ“š å–å¾—æ¸ˆã¿è«–æ–‡")
            
            df_data = []
            for p in st.session_state.papers:
                authors_list = p['authors']
                if isinstance(authors_list, list):
                    authors_str = ', '.join(authors_list[:2])
                    if len(authors_list) > 2:
                        authors_str += '...'
                else:
                    authors_str = authors_list[:50]
                
                df_data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': p['title'][:60] + '...' if len(p['title']) > 60 else p['title'],
                    'è‘—è€…': authors_str,
                    'å¹´': p['year'],
                    'ã‚½ãƒ¼ã‚¹': p.get('source', 'N/A'),
                    'å¼•ç”¨': p.get('citations', 0)
                })
            
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

            # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = pd.DataFrame(st.session_state.papers).to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

            # çµ±è¨ˆæƒ…å ±
            st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ç·è«–æ–‡æ•°", len(st.session_state.papers))
            
            with col2:
                years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A']
                if years:
                    year_counts = Counter(years)
                    most_common_year = year_counts.most_common(1)[0][0]
                    st.metric("æœ€å¤šç™ºè¡¨å¹´", most_common_year)
            
            with col3:
                if years:
                    st.metric("æœ€æ–°å¹´", max(years))
            
            with col4:
                total_citations = sum([p.get('citations', 0) for p in st.session_state.papers])
                st.metric("ç·å¼•ç”¨æ•°", total_citations)

        else:
            st.info("ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
