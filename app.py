"""
è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ 
"""
import streamlit as st
import sys
import os
from datetime import datetime
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import time
import requests
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”¬",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'papers' not in st.session_state:
    st.session_state.papers = []
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}


# ==================== PubMed Crawler ====================
class PubMedCrawler:
    """PubMed APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self, email: str = "user@example.com"):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = email

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        papers = []
        try:
            # æ¤œç´¢ã—ã¦PubMed IDã‚’å–å¾—
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'relevance'
            }

            if year_from:
                current_year = datetime.now().year
                search_params['datetype'] = 'pdat'
                search_params['mindate'] = f"{year_from}/01/01"
                search_params['maxdate'] = f"{current_year}/12/31"

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])

            if not id_list:
                return papers

            # è©³ç´°æƒ…å ±ã‚’å–å¾—
            fetch_url = f"{self.base_url}efetch.fcgi"
            batch_size = 5
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                ids_str = ','.join(batch_ids)

                fetch_params = {
                    'db': 'pubmed',
                    'id': ids_str,
                    'retmode': 'xml',
                    'email': self.email
                }

                fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
                fetch_response.raise_for_status()

                root = ET.fromstring(fetch_response.content)

                for article in root.findall('.//PubmedArticle'):
                    try:
                        paper_info = self._extract_paper_info(article, keyword)
                        papers.append(paper_info)
                    except Exception as e:
                        continue

                time.sleep(0.5)

            return papers

        except Exception as e:
            st.error(f"PubMed API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def _extract_paper_info(self, article_xml, keyword: str) -> Dict:
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = article_xml.find('.//ArticleTitle')
        title = title_elem.text if title_elem is not None else 'N/A'

        # è‘—è€…
        authors = []
        for author in article_xml.findall('.//Author'):
            lastname = author.find('LastName')
            forename = author.find('ForeName')
            if lastname is not None:
                name = lastname.text
                if forename is not None:
                    name = f"{forename.text} {name}"
                authors.append(name)

        # ç™ºè¡¨å¹´
        year_elem = article_xml.find('.//PubDate/Year')
        year = year_elem.text if year_elem is not None else 'N/A'

        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ
        abstract_texts = []
        for abstract in article_xml.findall('.//AbstractText'):
            if abstract.text:
                abstract_texts.append(abstract.text)
        abstract = ' '.join(abstract_texts) if abstract_texts else 'N/A'

        # ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å
        journal_elem = article_xml.find('.//Journal/Title')
        venue = journal_elem.text if journal_elem is not None else 'N/A'

        # PubMed ID
        pmid_elem = article_xml.find('.//PMID')
        pmid = pmid_elem.text if pmid_elem is not None else ''
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else 'N/A'

        return {
            'title': title,
            'authors': authors,
            'year': year,
            'abstract': abstract,
            'venue': venue,
            'url': url,
            'pmid': pmid,
            'citations': 0,
            'crawled_at': datetime.now().isoformat(),
            'keyword': keyword,
            'source': 'PubMed'
        }

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'date',
                'reldate': days
            }

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            ids_str = ','.join(id_list)

            fetch_params = {
                'db': 'pubmed',
                'id': ids_str,
                'retmode': 'xml',
                'email': self.email
            }

            fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
            fetch_response.raise_for_status()

            root = ET.fromstring(fetch_response.content)

            for article in root.findall('.//PubmedArticle'):
                try:
                    paper_info = self._extract_paper_info(article, keyword)
                    papers.append(paper_info)
                except:
                    continue

            return papers

        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª ====================
def main():
    st.title("ğŸ”¬ Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("PubMedã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ãƒ»è¦ç´„ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™")
    st.markdown("---")

    # ã‚¿ãƒ–
    tab1, tab2, tab3 = st.tabs(["ğŸ“š è«–æ–‡æ¤œç´¢", "ğŸ“Š ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿"])

    # ã‚¿ãƒ–1: è«–æ–‡æ¤œç´¢
    with tab1:
        st.header("è«–æ–‡æ¤œç´¢")

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                placeholder="ä¾‹: mass spectrometry proteomics",
                help="PubMedã§æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›"
            )
        with col2:
            max_results = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=50, value=10)

        col3, col4 = st.columns(2)
        with col3:
            year_from = st.number_input("æ¤œç´¢é–‹å§‹å¹´", min_value=2000, max_value=2030, value=2024)
        with col4:
            search_mode = st.selectbox("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["é€šå¸¸æ¤œç´¢", "æœ€è¿‘ã®è«–æ–‡ï¼ˆç›´è¿‘7æ—¥ï¼‰"])

        if st.button("ğŸ” æ¤œç´¢é–‹å§‹", type="primary"):
            if not query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                with st.spinner("è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                    try:
                        crawler = PubMedCrawler()

                        if search_mode == "é€šå¸¸æ¤œç´¢":
                            papers = crawler.search_papers(query, max_results, year_from)
                        else:
                            papers = crawler.get_recent_papers(query, days=7, max_results=max_results)

                        if papers:
                            st.session_state.papers = papers
                            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆã‚½ãƒ¼ã‚¹: PubMedï¼‰")

                            # çµæœã‚’è¡¨ç¤º
                            for i, paper in enumerate(papers[:10], 1):
                                with st.expander(f"ğŸ“„ {i}. {paper['title'][:80]}..."):
                                    authors_str = ', '.join(paper['authors'][:3]) if len(paper['authors']) > 3 else ', '.join(paper['authors'])
                                    st.markdown(f"**è‘—è€…**: {authors_str}")
                                    st.markdown(f"**ç™ºè¡¨å¹´**: {paper['year']} | **ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**: {paper['venue']}")
                                    st.markdown(f"**URL**: [{paper['url']}]({paper['url']})")
                                    if paper['abstract'] != 'N/A':
                                        st.markdown(f"**è¦æ—¨**: {paper['abstract'][:400]}...")
                        else:
                            st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # ã‚¿ãƒ–2: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    with tab2:
        st.header("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ")

        if st.session_state.papers:
            if st.button("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ"):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    # å…¨è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦æ—¨ã‚’çµåˆ
                    text = " ".join([
                        f"{p['title']} {p['abstract']}"
                        for p in st.session_state.papers
                        if p['abstract'] != 'N/A'
                    ])

                    if text:
                        wordcloud = WordCloud(
                            width=1200,
                            height=600,
                            background_color='white',
                            colormap='viridis',
                            max_words=100
                        ).generate(text)

                        fig, ax = plt.subplots(figsize=(15, 7))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        st.success("âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–3: ä¿å­˜ãƒ‡ãƒ¼ã‚¿
    with tab3:
        st.header("ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿")

        if st.session_state.papers:
            st.subheader("ğŸ“š å–å¾—æ¸ˆã¿è«–æ–‡")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            df_data = []
            for p in st.session_state.papers:
                authors_str = ', '.join(p['authors'][:2]) if len(p['authors']) > 2 else ', '.join(p['authors'])
                df_data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': p['title'][:60] + '...' if len(p['title']) > 60 else p['title'],
                    'è‘—è€…': authors_str,
                    'å¹´': p['year'],
                    'ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«': p['venue'][:40] + '...' if len(p.get('venue', '')) > 40 else p.get('venue', 'N/A')
                })
            
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

            # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = pd.DataFrame(st.session_state.papers).to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"papers_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )

            # çµ±è¨ˆæƒ…å ±
            st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("ç·è«–æ–‡æ•°", len(st.session_state.papers))
            
            with col2:
                years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A']
                if years:
                    year_counts = Counter(years)
                    most_common_year = year_counts.most_common(1)[0][0]
                    st.metric("æœ€å¤šç™ºè¡¨å¹´", most_common_year)
            
            with col3:
                if years:
                    st.metric("æœ€æ–°å¹´", max(years))

        else:
            st.info("ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„ã€‚")

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.sidebar.markdown("""
    1. æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›
    2. å–å¾—ä»¶æ•°ã¨å¹´ã‚’è¨­å®š
    3. ã€Œæ¤œç´¢é–‹å§‹ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
    4. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã§å¯è¦–åŒ–
    5. CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½
    """)
    st.sidebar.markdown("---")
    st.sidebar.info("ğŸ’¡ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã®ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¾ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹ã¨ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚")


if __name__ == "__main__":
    main()
