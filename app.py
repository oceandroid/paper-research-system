"""
Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ç‰ˆãƒ»å…¨ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿ï¼‰
"""
import streamlit as st
import sys
import os
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib
import pandas as pd
from collections import Counter
import time
import requests
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET
import re
import networkx as nx
import json

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”¬",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'papers' not in st.session_state:
    st.session_state.papers = []
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ""


# ==================== PubMed Crawler ====================
class PubMedCrawler:
    def __init__(self, email: str = "user@example.com"):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = email

    def search_papers(self, keyword: str, max_results: int = 20, year_from: Optional[int] = None) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'relevance'
            }

            if year_from:
                current_year = datetime.now().year
                search_params['datetype'] = 'pdat'
                search_params['mindate'] = f"{year_from}/01/01"
                search_params['maxdate'] = f"{current_year}/12/31"

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            batch_size = 5
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                ids_str = ','.join(batch_ids)

                fetch_params = {'db': 'pubmed', 'id': ids_str, 'retmode': 'xml', 'email': self.email}
                fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
                fetch_response.raise_for_status()
                root = ET.fromstring(fetch_response.content)

                for article in root.findall('.//PubmedArticle'):
                    try:
                        paper_info = self._extract_paper_info(article, keyword)
                        papers.append(paper_info)
                    except:
                        continue
                time.sleep(0.5)

            return papers
        except Exception as e:
            st.error(f"PubMed API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def _extract_paper_info(self, article_xml, keyword: str) -> Dict:
        title_elem = article_xml.find('.//ArticleTitle')
        title = title_elem.text if title_elem is not None else 'N/A'

        authors = []
        for author in article_xml.findall('.//Author'):
            lastname = author.find('LastName')
            forename = author.find('ForeName')
            if lastname is not None:
                name = lastname.text
                if forename is not None:
                    name = f"{forename.text} {name}"
                authors.append(name)

        year_elem = article_xml.find('.//PubDate/Year')
        year = year_elem.text if year_elem is not None else 'N/A'

        abstract_texts = []
        for abstract in article_xml.findall('.//AbstractText'):
            if abstract.text:
                abstract_texts.append(abstract.text)
        abstract = ' '.join(abstract_texts) if abstract_texts else 'N/A'

        journal_elem = article_xml.find('.//Journal/Title')
        venue = journal_elem.text if journal_elem is not None else 'N/A'

        pmid_elem = article_xml.find('.//PMID')
        pmid = pmid_elem.text if pmid_elem is not None else ''
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else 'N/A'

        return {
            'title': title, 'authors': authors, 'year': year, 'abstract': abstract,
            'venue': venue, 'url': url, 'pmid': pmid, 'citations': 0,
            'crawled_at': datetime.now().isoformat(), 'keyword': keyword, 'source': 'PubMed'
        }

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed', 'term': keyword, 'retmax': max_results,
                'retmode': 'json', 'email': self.email, 'sort': 'date', 'reldate': days
            }
            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            ids_str = ','.join(id_list)
            fetch_params = {'db': 'pubmed', 'id': ids_str, 'retmode': 'xml', 'email': self.email}
            fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.content)

            for article in root.findall('.//PubmedArticle'):
                try:
                    paper_info = self._extract_paper_info(article, keyword)
                    papers.append(paper_info)
                except:
                    continue
            return papers
        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers


# ==================== Semantic Scholar Crawler ====================
class SemanticScholarCrawler:
    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.headers = {'User-Agent': 'Mozilla/5.0'}

    def search_papers(self, keyword: str, max_results: int = 20, year_from: Optional[int] = None) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}/paper/search"
            limit_per_request = min(max_results, 10)
            params = {
                'query': keyword, 'limit': limit_per_request,
                'fields': 'title,authors,year,abstract,venue,citationCount,externalIds,url'
            }
            if year_from:
                params['year'] = f"{year_from}-"

            max_retries = 3
            for attempt in range(max_retries):
                try:
                    time.sleep(1)
                    response = requests.get(search_url, params=params, headers=self.headers, timeout=15)
                    if response.status_code == 429:
                        wait_time = 5 * (attempt + 1)
                        st.warning(f"Rate limitæ¤œå‡ºã€‚{wait_time}ç§’å¾…æ©Ÿä¸­...")
                        time.sleep(wait_time)
                        continue
                    response.raise_for_status()
                    data = response.json()
                    break
                except requests.exceptions.HTTPError as e:
                    if attempt == max_retries - 1:
                        raise
                    time.sleep(5)
                    continue

            for paper_data in data.get('data', []):
                try:
                    authors = [author['name'] for author in paper_data.get('authors', [])]
                    year = paper_data.get('year', 'N/A')
                    external_ids = paper_data.get('externalIds', {})
                    paper_id = paper_data.get('paperId', '')
                    url = f"https://www.semanticscholar.org/paper/{paper_id}"
                    if external_ids.get('DOI'):
                        url = f"https://doi.org/{external_ids['DOI']}"

                    paper_info = {
                        'title': paper_data.get('title', 'N/A'), 'authors': authors,
                        'year': str(year) if year else 'N/A',
                        'abstract': paper_data.get('abstract') or 'N/A',
                        'venue': paper_data.get('venue') or 'N/A', 'url': url,
                        'citations': paper_data.get('citationCount', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword, 'source': 'Semantic Scholar'
                    }
                    papers.append(paper_info)
                except:
                    continue
            return papers
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                st.error("Semantic Scholar API Rate limitã«é”ã—ã¾ã—ãŸã€‚æ•°åˆ†å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            else:
                st.error(f"Semantic Scholar API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers
        except Exception as e:
            st.error(f"Semantic Scholar API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        return self.search_papers(keyword, max_results, year_from=current_year)


# ==================== Google Scholar Crawler ====================
class ScholarCrawler:
    def __init__(self):
        self.results = []
        try:
            from scholarly import scholarly, ProxyGenerator
            self.scholarly = scholarly
            try:
                pg = ProxyGenerator()
                pg.FreeProxies()
                scholarly.use_proxy(pg)
            except:
                pass
        except ImportError:
            st.warning("scholarly ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            self.scholarly = None

    def search_papers(self, keyword: str, max_results: int = 20, year_from: Optional[int] = None) -> List[Dict]:
        if not self.scholarly:
            st.error("Google Scholaræ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []
        papers = []
        try:
            search_query = keyword
            if year_from:
                search_query = f"{keyword} after:{year_from}"
            search_results = self.scholarly.search_pubs(search_query)
            count = 0
            for result in search_results:
                if count >= max_results:
                    break
                try:
                    paper_info = {
                        'title': result.get('bib', {}).get('title', 'N/A'),
                        'authors': result.get('bib', {}).get('author', []),
                        'year': result.get('bib', {}).get('pub_year', 'N/A'),
                        'abstract': result.get('bib', {}).get('abstract', 'N/A'),
                        'venue': result.get('bib', {}).get('venue', 'N/A'),
                        'url': result.get('pub_url', result.get('eprint_url', 'N/A')),
                        'citations': result.get('num_citations', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword, 'source': 'Google Scholar'
                    }
                    papers.append(paper_info)
                    count += 1
                    time.sleep(2)
                except:
                    continue
            return papers
        except Exception as e:
            st.error(f"Google Scholar ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        target_date = datetime.now() - timedelta(days=days)
        year_from = target_date.year
        return self.search_papers(keyword, max_results, year_from)


# ==================== AIè¦ç´„ï¼ˆGemini APIï¼‰ ====================
def summarize_papers_with_gemini(papers: List[Dict], api_key: str) -> Dict[str, str]:
    """Gemini APIã§è«–æ–‡ã‚’ä¸€æ‹¬è¦ç´„"""
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦æœ€é©ãªã‚‚ã®ã‚’é¸æŠ
        try:
            model = genai.GenerativeModel('gemini-1.5-pro-latest')
        except:
            try:
                model = genai.GenerativeModel('gemini-pro')
            except:
                st.error("åˆ©ç”¨å¯èƒ½ãªGeminiãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {}
        
        summaries = {}
        
        for i, paper in enumerate(papers):
            try:
                if paper['abstract'] == 'N/A':
                    summaries[paper['title']] = "è¦æ—¨ãŒãªã„ãŸã‚è¦ç´„ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                    continue
                
                if isinstance(paper['authors'], list):
                    authors_str = ', '.join(paper['authors'][:3])
                else:
                    authors_str = paper['authors']
                
                prompt = f"""ä»¥ä¸‹ã®è«–æ–‡ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰ã€‚
Mass Spectrometryåˆ†é‡ã®ç ”ç©¶è€…å‘ã‘ã«ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ¼ã•ãˆã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']}
è‘—è€…: {authors_str}
å¹´: {paper['year']}
è¦æ—¨: {paper['abstract'][:1000]}

è¦ç´„:"""
                
                response = model.generate_content(prompt)
                summary = response.text
                summaries[paper['title']] = summary
                time.sleep(2)
                
            except Exception as e:
                summaries[paper['title']] = f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"
                continue
        
        return summaries
    except Exception as e:
        st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


# ==================== ãƒ†ã‚­ã‚¹ãƒˆè§£æ ====================
def extract_keywords(text: str, min_length: int = 4, top_n: int = 50) -> List[str]:
    words = re.findall(r'\b[a-zA-Z]{' + str(min_length) + r',}\b', text.lower())
    stop_words = {
        'this', 'that', 'with', 'from', 'were', 'been', 'have', 'has', 'had',
        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
        'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all',
        'was', 'said', 'them', 'than', 'find', 'also', 'made',
        'when', 'what', 'which', 'their', 'these', 'those', 'such', 'into',
        'through', 'during', 'before', 'after', 'about', 'between', 'under'
    }
    filtered_words = [w for w in words if w not in stop_words]
    word_counts = Counter(filtered_words)
    return [word for word, _ in word_counts.most_common(top_n)]


def build_cooccurrence_network(papers: List[Dict], top_keywords: int = 30, window_size: int = 10):
    all_text = " ".join([f"{p['title']} {p['abstract']}" for p in papers if p['abstract'] != 'N/A'])
    keywords = extract_keywords(all_text, min_length=5, top_n=top_keywords)
    cooccurrence = Counter()
    
    for paper in papers:
        text = f"{paper['title']} {paper['abstract']}"
        words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
        for i, word1 in enumerate(words):
            if word1 not in keywords:
                continue
            for j in range(i + 1, min(i + window_size, len(words))):
                word2 = words[j]
                if word2 in keywords and word1 != word2:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] += 1
    return keywords, cooccurrence


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª ====================
def main():
    st.title("ğŸ”¬ Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("é«˜åº¦ãªè«–æ–‡åˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰è§£æãƒ»AIè¦ç´„ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")

    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        gemini_key = st.text_input(
            "Google Gemini API Key",
            type="password",
            value=st.session_state.gemini_api_key,
            help="https://makersuite.google.com/app/apikey"
        )
        if gemini_key:
            st.session_state.gemini_api_key = gemini_key
            st.success("âœ… APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿")

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“š è«–æ–‡æ¤œç´¢", "ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰", "ğŸ¤– AIä¸€æ‹¬è¦ç´„",
        "ğŸ“Š ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿"
    ])

    # ã‚¿ãƒ–1: è«–æ–‡æ¤œç´¢
    with tab1:
        st.header("è«–æ–‡æ¤œç´¢")
        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            ["PubMedï¼ˆåŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦ï¼‰", "Semantic Scholarï¼ˆå…¨åˆ†é‡ãƒ»å¼•ç”¨æ•°ã‚ã‚Šï¼‰", "Google Scholarï¼ˆãƒ–ãƒ­ãƒƒã‚¯æ³¨æ„ï¼‰"]
        )

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹: mass spectrometry proteomics")
        with col2:
            max_results = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=20, value=10)

        col3, col4 = st.columns(2)
        with col3:
            year_from = st.number_input("æ¤œç´¢é–‹å§‹å¹´", min_value=2000, max_value=2030, value=2015)
        with col4:
            search_mode = st.selectbox("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["é€šå¸¸æ¤œç´¢", "æœ€è¿‘ã®è«–æ–‡ï¼ˆç›´è¿‘7æ—¥ï¼‰"])

        if st.button("ğŸ” æ¤œç´¢é–‹å§‹", type="primary"):
            if not query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                with st.spinner(f"{data_source}ã§è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                    try:
                        if "PubMed" in data_source:
                            crawler = PubMedCrawler()
                        elif "Semantic Scholar" in data_source:
                            crawler = SemanticScholarCrawler()
                        else:
                            crawler = ScholarCrawler()

                        if search_mode == "é€šå¸¸æ¤œç´¢":
                            papers = crawler.search_papers(query, max_results, year_from)
                        else:
                            papers = crawler.get_recent_papers(query, days=7, max_results=max_results)

                        if papers:
                            st.session_state.papers = papers
                            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸ")

                            for i, paper in enumerate(papers[:10], 1):
                                with st.expander(f"ğŸ“„ {i}. {paper['title'][:80]}..."):
                                    authors_list = paper['authors']
                                    if isinstance(authors_list, list):
                                        authors_str = ', '.join(authors_list[:3])
                                        if len(authors_list) > 3:
                                            authors_str += f" ä»–{len(authors_list) - 3}å"
                                    else:
                                        authors_str = authors_list
                                    
                                    st.markdown(f"**è‘—è€…**: {authors_str}")
                                    st.markdown(f"**ç™ºè¡¨å¹´**: {paper['year']} | **ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**: {paper.get('venue', 'N/A')}")
                                    if paper.get('citations', 0) > 0:
                                        st.markdown(f"**å¼•ç”¨æ•°**: {paper['citations']}")
                                    st.markdown(f"**URL**: [{paper['url']}]({paper['url']})")
                                    if paper['abstract'] != 'N/A':
                                        st.markdown(f"**è¦æ—¨**: {paper['abstract'][:400]}...")
                        else:
                            st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

    # ã‚¿ãƒ–2: ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰
    with tab2:
        st.header("ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
        if st.session_state.papers:
            st.subheader("Year-wise Publication Trend")
            years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A' and str(p['year']).isdigit()]
            
            if years:
                year_counts = Counter(years)
                year_df = pd.DataFrame(list(year_counts.items()), columns=['Year', 'Count']).sort_values('Year')
                
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(year_df['Year'], year_df['Count'], marker='o', linewidth=2, markersize=8)
                ax.set_xlabel('Year', fontsize=12)
                ax.set_ylabel('Number of Papers', fontsize=12)
                ax.set_title('Publication Trend by Year', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Papers", len(years))
                with col2:
                    peak_year = year_counts.most_common(1)[0][0]
                    st.metric("Peak Year", peak_year)
                with col3:
                    avg_per_year = len(years) / len(year_counts) if year_counts else 0
                    st.metric("Avg/Year", f"{avg_per_year:.1f}")
            else:
                st.warning("å¹´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–3: AIä¸€æ‹¬è¦ç´„
    with tab3:
        st.header("ğŸ¤– AIä¸€æ‹¬è¦ç´„")
        if st.session_state.papers:
            if not st.session_state.gemini_api_key:
                st.warning("âš ï¸ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Gemini API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
                st.markdown("[Google AI Studio](https://makersuite.google.com/app/apikey)ã§ç„¡æ–™å–å¾—")
            else:
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãŒã‚ã‚Šã¾ã™")
                with col2:
                    max_summarize = st.number_input("è¦ç´„ã™ã‚‹ä»¶æ•°", 1, min(10, len(st.session_state.papers)), 5)
                
                if st.button("ğŸ¤– AIè¦ç´„ã‚’é–‹å§‹", type="primary"):
                    papers_to_summarize = st.session_state.papers[:max_summarize]
                    with st.spinner(f"{len(papers_to_summarize)}ä»¶ã®è«–æ–‡ã‚’è¦ç´„ä¸­..."):
                        summaries = summarize_papers_with_gemini(papers_to_summarize, st.session_state.gemini_api_key)
                        if summaries:
                            st.session_state.summaries = summaries
                            st.success(f"âœ… {len(summaries)}ä»¶ã®è«–æ–‡ã‚’è¦ç´„ã—ã¾ã—ãŸ")
                            for i, (title, summary) in enumerate(summaries.items(), 1):
                                with st.expander(f"ğŸ“ {i}. {title[:70]}..."):
                                    st.markdown(f"**è¦ç´„**: {summary}")
                                    original_paper = next((p for p in papers_to_summarize if p['title'] == title), None)
                                    if original_paper:
                                        st.markdown(f"**URL**: [{original_paper['url']}]({original_paper['url']})")
                        else:
                            st.error("è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–4: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    with tab4:
        st.header("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ")
        if st.session_state.papers:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™")
            with col2:
                max_words = st.slider("æœ€å¤§å˜èªæ•°", 30, 200, 100)

            if st.button("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ"):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    text = " ".join([f"{p['title']} {p['abstract']}" for p in st.session_state.papers if p['abstract'] != 'N/A'])
                    if text:
                        wordcloud = WordCloud(width=1200, height=600, background_color='white', colormap='viridis', max_words=max_words).generate(text)
                        fig, ax = plt.subplots(figsize=(15, 7))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        st.success("âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–5: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    with tab5:
        st.header("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ")
        if st.session_state.papers:
            col1, col2, col3 = st.columns(3)
            with col1:
                top_keywords = st.slider("è¡¨ç¤ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 10, 50, 30)
            with col2:
                window_size = st.slider("å…±èµ·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦", 5, 20, 10)
            with col3:
                min_cooccurrence = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 10, 2)

            if st.button("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆ"):
                with st.spinner("è§£æä¸­..."):
                    keywords, cooccurrence = build_cooccurrence_network(st.session_state.papers, top_keywords, window_size)
                    G = nx.Graph()
                    for (word1, word2), count in cooccurrence.items():
                        if count >= min_cooccurrence:
                            G.add_edge(word1, word2, weight=count)

                    if len(G.nodes()) > 0:
                        pos = nx.spring_layout(G, k=0.5, iterations=50)
                        fig, ax = plt.subplots(figsize=(16, 12))
                        node_sizes = [G.degree(node) * 300 for node in G.nodes()]
                        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', alpha=0.7, ax=ax)
                        edges = G.edges()
                        weights = [G[u][v]['weight'] for u, v in edges]
                        max_weight = max(weights) if weights else 1
                        nx.draw_networkx_edges(G, pos, width=[w / max_weight * 5 for w in weights], alpha=0.3, ax=ax)
                        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold', ax=ax)
                        ax.axis('off')
                        ax.set_title(f"Co-occurrence Network (Nodes: {len(G.nodes())}, Edges: {len(G.edges())})", fontsize=16)
                        st.pyplot(fig)
                        
                        st.subheader("Network Statistics")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Nodes", len(G.nodes()))
                        with col2:
                            st.metric("Edges", len(G.edges()))
                        with col3:
                            if len(G.nodes()) > 0:
                                avg_degree = sum(dict(G.degree()).values()) / len(G.nodes())
                                st.metric("Avg Degree", f"{avg_degree:.2f}")
                        st.success("âœ… å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–6: ä¿å­˜ãƒ‡ãƒ¼ã‚¿
    with tab6:
        st.header("ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿")
        if st.session_state.papers:
            st.subheader("ğŸ† å¼•ç”¨æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆTop 10ï¼‰")
            papers_with_citations = [p for p in st.session_state.papers if p.get('citations', 0) > 0]
            if papers_with_citations:
                sorted_papers = sorted(papers_with_citations, key=lambda x: x.get('citations', 0), reverse=True)[:10]
                for i, paper in enumerate(sorted_papers, 1):
                    col1, col2 = st.columns([5, 1])
                    with col1:
                        st.markdown(f"**{i}. {paper['title'][:70]}...**")
                        authors_str = ', '.join(paper['authors'][:2]) if isinstance(paper['authors'], list) else paper['authors']
                        st.caption(f"{authors_str} ({paper['year']})")
                    with col2:
                        st.metric("Citations", paper['citations'])
            else:
                st.info("å¼•ç”¨æ•°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            st.markdown("---")
            st.subheader("ğŸ“š å–å¾—æ¸ˆã¿è«–æ–‡")
            df_data = []
            for p in st.session_state.papers:
                authors_list = p['authors']
                if isinstance(authors_list, list):
                    authors_str = ', '.join(authors_list[:2])
                    if len(authors_list) > 2:
                        authors_str += '...'
                else:
                    authors_str = authors_list[:50]
                df_data.append({
                    'Title': p['title'][:60] + '...' if len(p['title']) > 60 else p['title'],
                    'Authors': authors_str, 'Year': p['year'],
                    'Source': p.get('source', 'N/A'), 'Citations': p.get('citations', 0)
                })
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

            csv = pd.DataFrame(st.session_state.papers).to_csv(index=False, encoding='utf-8-sig')
            st.download_button(label="ğŸ“¥ CSV Download", data=csv, file_name=f"papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv", mime="text/csv")

            st.subheader("ğŸ“Š Statistics")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Papers", len(st.session_state.papers))
            with col2:
                years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A']
                if years:
                    year_counts = Counter(years)
                    most_common_year = year_counts.most_common(1)[0][0]
                    st.metric("Most Common Year", most_common_year)
            with col3:
                if years:
                    st.metric("Latest Year", max(years))
            with col4:
                total_citations = sum([p.get('citations', 0) for p in st.session_state.papers])
                st.metric("Total Citations", total_citations)
        else:
            st.info("ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
