"""
Streamlitã«ã‚ˆã‚‹è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ã®UIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
Streamlit Cloudç”¨ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ç½®ç‰ˆï¼‰
"""
import streamlit as st
import sys
import os
import json
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter

# ãƒ‘ã‚¹ã‚’è¿½åŠ ï¼ˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å®Ÿè¡Œã•ã‚Œã‚‹å ´åˆï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from database.database import DatabaseManager
from crawler.scholar_crawler import ScholarCrawler
from crawler.pubmed_crawler import PubMedCrawler
from analyzer.llm_analyzer import LLMAnalyzer


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“š",
    layout="wide"
)


@st.cache_resource
def get_db():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³"""
    return DatabaseManager("papers.db")


@st.cache_resource
def get_analyzer():
    """LLMè§£æå™¨ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³"""
    return LLMAnalyzer()


def main():
    st.title("ğŸ“š è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
        page = st.radio(
            "ãƒšãƒ¼ã‚¸é¸æŠ",
            ["ãƒ›ãƒ¼ãƒ ", "è«–æ–‡æ¤œç´¢ãƒ»ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–²è¦§", "è§£æãƒ»å¯è¦–åŒ–", "è¨­å®š"]
        )

    db = get_db()

    # ãƒšãƒ¼ã‚¸è¡¨ç¤º
    if page == "ãƒ›ãƒ¼ãƒ ":
        show_home(db)
    elif page == "è«–æ–‡æ¤œç´¢ãƒ»ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°":
        show_crawling_page(db)
    elif page == "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–²è¦§":
        show_database_page(db)
    elif page == "è§£æãƒ»å¯è¦–åŒ–":
        show_analysis_page(db)
    elif page == "è¨­å®š":
        show_settings_page()


def show_home(db):
    """ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"""
    st.header("ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")

    # çµ±è¨ˆæƒ…å ±
    papers = db.get_all_papers()
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("ç·è«–æ–‡æ•°", len(papers))

    with col2:
        recent_logs = db.get_recent_crawls(limit=1)
        last_crawl = recent_logs[0].executed_at.strftime("%Y-%m-%d %H:%M") if recent_logs else "æœªå®Ÿè¡Œ"
        st.metric("æœ€çµ‚ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°", last_crawl)

    with col3:
        # æœ€æ–°ã®è«–æ–‡
        if papers:
            latest_year = max([p.year for p in papers if p.year != 'N/A'])
            st.metric("æœ€æ–°è«–æ–‡å¹´", latest_year)

    st.markdown("---")

    # æœ€è¿‘ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å±¥æ­´
    st.subheader("ğŸ”„ æœ€è¿‘ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å±¥æ­´")
    logs = db.get_recent_crawls(limit=5)

    if logs:
        log_data = []
        for log in logs:
            log_data.append({
                "å®Ÿè¡Œæ—¥æ™‚": log.executed_at.strftime("%Y-%m-%d %H:%M:%S"),
                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": log.keyword,
                "å–å¾—æ•°": log.papers_count,
                "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": log.status
            })
        st.dataframe(pd.DataFrame(log_data), use_container_width=True)
    else:
        st.info("ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")

    st.markdown("---")

    # æœ€æ–°è«–æ–‡
    st.subheader("ğŸ“„ æœ€æ–°ã®è«–æ–‡ï¼ˆ5ä»¶ï¼‰")
    recent_papers = db.get_all_papers(limit=5)

    if recent_papers:
        for paper in recent_papers:
            with st.expander(f"**{paper.title}** ({paper.year})"):
                st.write(f"**è‘—è€…:** {paper.authors}")
                st.write(f"**æ²è¼‰:** {paper.venue}")
                st.write(f"**å¼•ç”¨æ•°:** {paper.citations}")
                st.write(f"**URL:** {paper.url}")
                if paper.abstract != 'N/A':
                    st.write(f"**æ¦‚è¦:** {paper.abstract[:300]}...")
    else:
        st.info("ã¾ã è«–æ–‡ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€Œè«–æ–‡æ¤œç´¢ãƒ»ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã€ã‹ã‚‰è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚")


def show_crawling_page(db):
    """è«–æ–‡ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒšãƒ¼ã‚¸"""
    st.header("ğŸ” è«–æ–‡æ¤œç´¢ãƒ»ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°")

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
    data_source = st.radio(
        "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
        ["PubMedï¼ˆæ¨å¥¨ãƒ»å®‰å®šï¼‰", "Google Scholar"],
        help="PubMedã¯å…¬å¼APIã§å®‰å®šã€Google Scholarã¯ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š"
    )

    col1, col2 = st.columns(2)

    with col1:
        keyword = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="mass spectrometry")
        max_results = st.slider("å–å¾—ä»¶æ•°", min_value=5, max_value=50, value=10)

    with col2:
        year_from = st.number_input("æ¤œç´¢é–‹å§‹å¹´", min_value=2000, max_value=2030, value=2024)
        search_mode = st.selectbox("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["é€šå¸¸æ¤œç´¢", "æœ€è¿‘ã®è«–æ–‡ï¼ˆç›´è¿‘7æ—¥ï¼‰"])

    if st.button("ğŸš€ æ¤œç´¢é–‹å§‹", type="primary"):
        with st.spinner("è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’é¸æŠ
                if data_source == "PubMedï¼ˆæ¨å¥¨ãƒ»å®‰å®šï¼‰":
                    crawler = PubMedCrawler(email="user@example.com")
                else:
                    crawler = ScholarCrawler()

                if search_mode == "é€šå¸¸æ¤œç´¢":
                    papers = crawler.search_papers(keyword, max_results, year_from)
                else:
                    papers = crawler.get_recent_papers(keyword, days=7, max_results=max_results)

                if papers:
                    st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆã‚½ãƒ¼ã‚¹: {data_source}ï¼‰")

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    saved_count = db.save_papers(papers)
                    db.log_crawl(keyword, saved_count, "success")

                    st.info(f"ğŸ’¾ {saved_count}ä»¶ã®æ–°è¦è«–æ–‡ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸ")

                    # çµæœã‚’è¡¨ç¤º
                    for i, paper in enumerate(papers[:5], 1):
                        with st.expander(f"{i}. {paper['title'][:80]}..."):
                            st.write(f"**è‘—è€…:** {', '.join(paper['authors'][:3]) if isinstance(paper['authors'], list) else paper['authors']}")
                            st.write(f"**å¹´:** {paper['year']}")
                            st.write(f"**å¼•ç”¨æ•°:** {paper['citations']}")
                            st.write(f"**ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«:** {paper.get('venue', 'N/A')}")
                            st.write(f"**URL:** [{paper['url']}]({paper['url']})")
                            if paper.get('abstract') and paper['abstract'] != 'N/A':
                                st.write(f"**è¦æ—¨:** {paper['abstract'][:300]}...")
                else:
                    st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    db.log_crawl(keyword, 0, "failed", "No papers found")

            except Exception as e:
                st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.info("ğŸ’¡ Google Scholarã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã€PubMedã‚’ãŠè©¦ã—ãã ã•ã„")
                db.log_crawl(keyword, 0, "failed", str(e))


def show_database_page(db):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–²è¦§ãƒšãƒ¼ã‚¸"""
    st.header("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–²è¦§")

    # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    col1, col2 = st.columns(2)
    with col1:
        keyword_filter = st.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", "")
    with col2:
        limit = st.number_input("è¡¨ç¤ºä»¶æ•°", min_value=10, max_value=100, value=20)

    # è«–æ–‡å–å¾—
    if keyword_filter:
        papers = db.get_papers_by_keyword(keyword_filter)
    else:
        papers = db.get_all_papers(limit=limit)

    st.write(f"**è¡¨ç¤ºä»¶æ•°:** {len(papers)}ä»¶")

    if papers:
        for paper in papers:
            with st.expander(f"**{paper.title}** ({paper.year})"):
                col1, col2 = st.columns([3, 1])

                with col1:
                    authors_list = json.loads(paper.authors) if paper.authors.startswith('[') else paper.authors
                    authors_str = ', '.join(authors_list) if isinstance(authors_list, list) else authors_list

                    st.write(f"**è‘—è€…:** {authors_str}")
                    st.write(f"**æ²è¼‰:** {paper.venue}")
                    st.write(f"**URL:** {paper.url}")
                    if paper.abstract != 'N/A':
                        st.write(f"**æ¦‚è¦:** {paper.abstract}")

                with col2:
                    st.metric("å¼•ç”¨æ•°", paper.citations)
                    st.write(f"ç™»éŒ²æ—¥: {paper.created_at.strftime('%Y-%m-%d')}")

                # è§£æãƒœã‚¿ãƒ³
                if st.button(f"ã“ã®è«–æ–‡ã‚’è§£æ", key=f"analyze_{paper.id}"):
                    analyze_single_paper(db, paper)
    else:
        st.info("è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“")


def analyze_single_paper(db, paper):
    """å˜ä¸€ã®è«–æ–‡ã‚’è§£æ"""
    analyzer = get_analyzer()

    with st.spinner("è«–æ–‡ã‚’è§£æä¸­..."):
        # è‘—è€…æƒ…å ±ã‚’æ•´å½¢
        authors_list = json.loads(paper.authors) if paper.authors.startswith('[') else paper.authors
        authors_str = ', '.join(authors_list) if isinstance(authors_list, list) else authors_list

        analysis = analyzer.analyze_paper(
            title=paper.title,
            abstract=paper.abstract,
            authors=authors_str,
            year=paper.year
        )

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        db.save_analysis(paper.id, analysis)

        st.success("è§£æå®Œäº†ï¼")
        st.json(analysis)


def show_analysis_page(db):
    """è§£æãƒ»å¯è¦–åŒ–ãƒšãƒ¼ã‚¸"""
    st.header("ğŸ“Š è§£æãƒ»å¯è¦–åŒ–")

    papers = db.get_all_papers()

    if not papers:
        st.warning("è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšè«–æ–‡ã‚’ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚")
        return

    tab1, tab2, tab3 = st.tabs(["ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "çµ±è¨ˆåˆ†æ", "ä¸€æ‹¬è§£æ"])

    with tab1:
        st.subheader("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")

        # ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        all_text = " ".join([
            paper.abstract for paper in papers
            if paper.abstract and paper.abstract != 'N/A'
        ])

        if all_text:
            try:
                # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ
                wordcloud = WordCloud(
                    width=800,
                    height=400,
                    background_color='white',
                    colormap='viridis',
                    max_words=100
                ).generate(all_text)

                # è¡¨ç¤º
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.imshow(wordcloud, interpolation='bilinear')
                ax.axis('off')
                st.pyplot(fig)

            except Exception as e:
                st.error(f"ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.info("ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

    with tab2:
        st.subheader("ğŸ“ˆ çµ±è¨ˆåˆ†æ")

        # å¹´åˆ¥è«–æ–‡æ•°
        years = [p.year for p in papers if p.year != 'N/A']
        year_counts = Counter(years)

        if year_counts:
            st.write("**å¹´åˆ¥è«–æ–‡æ•°**")
            year_df = pd.DataFrame(
                list(year_counts.items()),
                columns=['å¹´', 'è«–æ–‡æ•°']
            ).sort_values('å¹´')
            st.bar_chart(year_df.set_index('å¹´'))

        # å¼•ç”¨æ•°ä¸Šä½
        st.write("**å¼•ç”¨æ•°ãƒˆãƒƒãƒ—10**")
        papers_sorted = sorted(papers, key=lambda x: x.citations, reverse=True)[:10]
        citation_data = [{
            'ã‚¿ã‚¤ãƒˆãƒ«': p.title[:50] + '...' if len(p.title) > 50 else p.title,
            'å¼•ç”¨æ•°': p.citations,
            'å¹´': p.year
        } for p in papers_sorted]
        st.dataframe(pd.DataFrame(citation_data), use_container_width=True)

    with tab3:
        st.subheader("ğŸ”¬ ä¸€æ‹¬è§£æ")
        st.write("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®è«–æ–‡ã‚’LLMã§ä¸€æ‹¬è§£æã—ã¾ã™")

        limit = st.slider("è§£æã™ã‚‹è«–æ–‡æ•°", min_value=1, max_value=20, value=5)

        if st.button("ä¸€æ‹¬è§£æã‚’é–‹å§‹", type="primary"):
            analyzer = get_analyzer()
            papers_to_analyze = db.get_all_papers(limit=limit)

            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, paper in enumerate(papers_to_analyze):
                status_text.text(f"è§£æä¸­ ({i+1}/{len(papers_to_analyze)}): {paper.title[:50]}...")

                # æ—¢ã«è§£ææ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
                existing_analysis = db.get_analysis_by_paper_id(paper.id)
                if existing_analysis:
                    continue

                # è‘—è€…æƒ…å ±ã‚’æ•´å½¢
                authors_list = json.loads(paper.authors) if paper.authors.startswith('[') else paper.authors
                authors_str = ', '.join(authors_list) if isinstance(authors_list, list) else authors_list

                # è§£æå®Ÿè¡Œ
                analysis = analyzer.analyze_paper(
                    title=paper.title,
                    abstract=paper.abstract,
                    authors=authors_str,
                    year=paper.year
                )

                # ä¿å­˜
                db.save_analysis(paper.id, analysis)

                progress_bar.progress((i + 1) / len(papers_to_analyze))

            status_text.text("è§£æå®Œäº†ï¼")
            st.success(f"{len(papers_to_analyze)}ä»¶ã®è«–æ–‡ã®è§£æãŒå®Œäº†ã—ã¾ã—ãŸ")


def show_settings_page():
    """è¨­å®šãƒšãƒ¼ã‚¸"""
    st.header("âš™ï¸ è¨­å®š")

    st.subheader("APIè¨­å®š")
    st.write("ç’°å¢ƒå¤‰æ•° `OPENAI_API_KEY` ã‚’è¨­å®šã—ã¦ãã ã•ã„")

    api_key = st.text_input("OpenAI APIã‚­ãƒ¼", type="password", placeholder="sk-...")

    if api_key:
        st.success("APIã‚­ãƒ¼ãŒå…¥åŠ›ã•ã‚Œã¾ã—ãŸï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ãã ã•ã„ï¼‰")

        # .envãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ä¾‹
        if st.button("ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦ä¿å­˜"):
            env_path = os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                '.env'
            )
            with open(env_path, 'w') as f:
                f.write(f"OPENAI_API_KEY={api_key}\n")
            st.success(f".envãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {env_path}")

    st.markdown("---")
    st.subheader("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š")
    st.write("å®šæœŸå®Ÿè¡Œã®è¨­å®šã¯ `scheduler/scheduler.py` ã‚’ç·¨é›†ã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
