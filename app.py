"""
Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰
- Semantic Scholar Rate limitå¯¾ç­–
- Gemini APIæœ€æ–°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
"""
import streamlit as st
import sys
import os
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import time
import requests
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET
import re
from itertools import combinations
import networkx as nx
import json

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”¬",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'papers' not in st.session_state:
    st.session_state.papers = []
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}
if 'gemini_api_key' not in st.session_state:
    st.session_state.gemini_api_key = ""


# ==================== PubMed Crawler ====================
class PubMedCrawler:
    """PubMed APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self, email: str = "user@example.com"):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = email

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'relevance'
            }

            if year_from:
                current_year = datetime.now().year
                search_params['datetype'] = 'pdat'
                search_params['mindate'] = f"{year_from}/01/01"
                search_params['maxdate'] = f"{current_year}/12/31"

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            batch_size = 5
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                ids_str = ','.join(batch_ids)

                fetch_params = {
                    'db': 'pubmed',
                    'id': ids_str,
                    'retmode': 'xml',
                    'email': self.email
                }

                fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
                fetch_response.raise_for_status()
                root = ET.fromstring(fetch_response.content)

                for article in root.findall('.//PubmedArticle'):
                    try:
                        paper_info = self._extract_paper_info(article, keyword)
                        papers.append(paper_info)
                    except:
                        continue
                time.sleep(0.5)

            return papers

        except Exception as e:
            st.error(f"PubMed API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def _extract_paper_info(self, article_xml, keyword: str) -> Dict:
        title_elem = article_xml.find('.//ArticleTitle')
        title = title_elem.text if title_elem is not None else 'N/A'

        authors = []
        for author in article_xml.findall('.//Author'):
            lastname = author.find('LastName')
            forename = author.find('ForeName')
            if lastname is not None:
                name = lastname.text
                if forename is not None:
                    name = f"{forename.text} {name}"
                authors.append(name)

        year_elem = article_xml.find('.//PubDate/Year')
        year = year_elem.text if year_elem is not None else 'N/A'

        abstract_texts = []
        for abstract in article_xml.findall('.//AbstractText'):
            if abstract.text:
                abstract_texts.append(abstract.text)
        abstract = ' '.join(abstract_texts) if abstract_texts else 'N/A'

        journal_elem = article_xml.find('.//Journal/Title')
        venue = journal_elem.text if journal_elem is not None else 'N/A'

        pmid_elem = article_xml.find('.//PMID')
        pmid = pmid_elem.text if pmid_elem is not None else ''
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else 'N/A'

        return {
            'title': title,
            'authors': authors,
            'year': year,
            'abstract': abstract,
            'venue': venue,
            'url': url,
            'pmid': pmid,
            'citations': 0,
            'crawled_at': datetime.now().isoformat(),
            'keyword': keyword,
            'source': 'PubMed'
        }

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'date',
                'reldate': days
            }

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            ids_str = ','.join(id_list)

            fetch_params = {
                'db': 'pubmed',
                'id': ids_str,
                'retmode': 'xml',
                'email': self.email
            }

            fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.content)

            for article in root.findall('.//PubmedArticle'):
                try:
                    paper_info = self._extract_paper_info(article, keyword)
                    papers.append(paper_info)
                except:
                    continue

            return papers

        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers


# ==================== Semantic Scholar Crawler (ä¿®æ­£ç‰ˆ) ====================
class SemanticScholarCrawler:
    """Semantic Scholar APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—ï¼ˆRate limitå¯¾ç­–ç‰ˆï¼‰"""

    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}/paper/search"
            
            # ä¸€åº¦ã«å°‘ãªã‚ã«å–å¾—ï¼ˆRate limitå¯¾ç­–ï¼‰
            limit_per_request = min(max_results, 10)
            
            params = {
                'query': keyword,
                'limit': limit_per_request,
                'fields': 'title,authors,year,abstract,venue,citationCount,externalIds,url'
            }

            if year_from:
                params['year'] = f"{year_from}-"

            # ãƒªãƒˆãƒ©ã‚¤å‡¦ç†
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    time.sleep(1)  # Rate limitå¯¾ç­–
                    
                    response = requests.get(
                        search_url, 
                        params=params, 
                        headers=self.headers,
                        timeout=15
                    )
                    
                    if response.status_code == 429:
                        # Rate limitã«å¼•ã£ã‹ã‹ã£ãŸå ´åˆ
                        wait_time = 5 * (attempt + 1)
                        st.warning(f"Rate limitæ¤œå‡ºã€‚{wait_time}ç§’å¾…æ©Ÿä¸­...")
                        time.sleep(wait_time)
                        continue
                    
                    response.raise_for_status()
                    data = response.json()
                    break
                    
                except requests.exceptions.HTTPError as e:
                    if attempt == max_retries - 1:
                        raise
                    time.sleep(5)
                    continue

            for paper_data in data.get('data', []):
                try:
                    authors = [author['name'] for author in paper_data.get('authors', [])]
                    year = paper_data.get('year', 'N/A')
                    
                    external_ids = paper_data.get('externalIds', {})
                    paper_id = paper_data.get('paperId', '')
                    url = f"https://www.semanticscholar.org/paper/{paper_id}"
                    
                    if external_ids.get('DOI'):
                        url = f"https://doi.org/{external_ids['DOI']}"

                    paper_info = {
                        'title': paper_data.get('title', 'N/A'),
                        'authors': authors,
                        'year': str(year) if year else 'N/A',
                        'abstract': paper_data.get('abstract') or 'N/A',
                        'venue': paper_data.get('venue') or 'N/A',
                        'url': url,
                        'citations': paper_data.get('citationCount', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword,
                        'source': 'Semantic Scholar'
                    }

                    papers.append(paper_info)

                except Exception as e:
                    continue

            return papers

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429:
                st.error("Semantic Scholar API Rate limitã«é”ã—ã¾ã—ãŸã€‚æ•°åˆ†å¾Œã«å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
                st.info("ğŸ’¡ ä»£ã‚ã‚Šã«PubMedã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
            else:
                st.error(f"Semantic Scholar API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers
        except Exception as e:
            st.error(f"Semantic Scholar API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        return self.search_papers(keyword, max_results, year_from=current_year)


# ==================== Google Scholar Crawler ====================
class ScholarCrawler:
    """Google Scholarã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self):
        self.results = []
        try:
            from scholarly import scholarly, ProxyGenerator
            self.scholarly = scholarly
            
            try:
                pg = ProxyGenerator()
                pg.FreeProxies()
                scholarly.use_proxy(pg)
            except:
                pass
        except ImportError:
            st.warning("scholarly ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            self.scholarly = None

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        if not self.scholarly:
            st.error("Google Scholaræ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")
            return []

        papers = []
        try:
            search_query = keyword
            if year_from:
                search_query = f"{keyword} after:{year_from}"

            search_results = self.scholarly.search_pubs(search_query)

            count = 0
            for result in search_results:
                if count >= max_results:
                    break

                try:
                    paper_info = {
                        'title': result.get('bib', {}).get('title', 'N/A'),
                        'authors': result.get('bib', {}).get('author', []),
                        'year': result.get('bib', {}).get('pub_year', 'N/A'),
                        'abstract': result.get('bib', {}).get('abstract', 'N/A'),
                        'venue': result.get('bib', {}).get('venue', 'N/A'),
                        'url': result.get('pub_url', result.get('eprint_url', 'N/A')),
                        'citations': result.get('num_citations', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword,
                        'source': 'Google Scholar'
                    }

                    papers.append(paper_info)
                    count += 1
                    time.sleep(2)

                except Exception as e:
                    continue

            return papers

        except Exception as e:
            st.error(f"Google Scholar ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ’¡ Google ScholarãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚Semantic Scholarã¾ãŸã¯PubMedã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        target_date = datetime.now() - timedelta(days=days)
        year_from = target_date.year
        return self.search_papers(keyword, max_results, year_from)


# ==================== AIè¦ç´„ï¼ˆGemini API - ä¿®æ­£ç‰ˆï¼‰ ====================
def summarize_papers_with_gemini(papers: List[Dict], api_key: str, language: str = "japanese") -> Dict[str, str]:
    """Gemini APIã§è«–æ–‡ã‚’ä¸€æ‹¬è¦ç´„ï¼ˆæœ€æ–°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰"""
    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        
        # æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆgemini-1.5-flashï¼‰
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        summaries = {}
        
        for i, paper in enumerate(papers):
            try:
                if paper['abstract'] == 'N/A':
                    summaries[paper['title']] = "è¦æ—¨ãŒãªã„ãŸã‚è¦ç´„ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                    continue
                
                # è‘—è€…æƒ…å ±ã‚’æ•´å½¢
                if isinstance(paper['authors'], list):
                    authors_str = ', '.join(paper['authors'][:3])
                else:
                    authors_str = paper['authors']
                
                prompt = f"""
ä»¥ä¸‹ã®è«–æ–‡ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆ200æ–‡å­—ç¨‹åº¦ï¼‰ã€‚
Mass Spectrometryåˆ†é‡ã®ç ”ç©¶è€…å‘ã‘ã«ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ¼ã•ãˆã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {paper['title']}
è‘—è€…: {authors_str}
å¹´: {paper['year']}
è¦æ—¨: {paper['abstract'][:1000]}

è¦ç´„:
"""
                
                response = model.generate_content(prompt)
                summary = response.text
                summaries[paper['title']] = summary
                
                time.sleep(2)  # APIåˆ¶é™å¯¾ç­–
                
            except Exception as e:
                summaries[paper['title']] = f"è¦ç´„ã‚¨ãƒ©ãƒ¼: {str(e)}"
                continue
        
        return summaries
    
    except Exception as e:
        st.error(f"Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        st.info("ğŸ’¡ APIã‚­ãƒ¼ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚https://makersuite.google.com/app/apikey")
        return {}


# ==================== ãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====================
def extract_keywords(text: str, min_length: int = 4, top_n: int = 50) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    words = re.findall(r'\b[a-zA-Z]{' + str(min_length) + r',}\b', text.lower())
    
    stop_words = {
        'this', 'that', 'with', 'from', 'were', 'been', 'have', 'has', 'had',
        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
        'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can',
        'was', 'said', 'them', 'been', 'than', 'find', 'also', 'made',
        'when', 'what', 'which', 'their', 'these', 'those', 'such', 'into',
        'through', 'during', 'before', 'after', 'about', 'between', 'under'
    }
    
    filtered_words = [w for w in words if w not in stop_words]
    word_counts = Counter(filtered_words)
    return [word for word, _ in word_counts.most_common(top_n)]


def build_cooccurrence_network(papers: List[Dict], top_keywords: int = 30, window_size: int = 10):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    all_text = " ".join([
        f"{p['title']} {p['abstract']}"
        for p in papers
        if p['abstract'] != 'N/A'
    ])
    
    keywords = extract_keywords(all_text, min_length=5, top_n=top_keywords)
    cooccurrence = Counter()
    
    for paper in papers:
        text = f"{paper['title']} {paper['abstract']}"
        words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
        
        for i, word1 in enumerate(words):
            if word1 not in keywords:
                continue
            
            for j in range(i + 1, min(i + window_size, len(words))):
                word2 = words[j]
                if word2 in keywords and word1 != word2:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] += 1
    
    return keywords, cooccurrence


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª ====================
def main():
    st.title("ğŸ”¬ Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("é«˜åº¦ãªè«–æ–‡åˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰è§£æãƒ»AIè¦ç´„ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: API Keyè¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        gemini_key = st.text_input(
            "Google Gemini API Key",
            type="password",
            value=st.session_state.gemini_api_key,
            help="https://makersuite.google.com/app/apikey ã§å–å¾—ï¼ˆç„¡æ–™ï¼‰"
        )
        if gemini_key:
            st.session_state.gemini_api_key = gemini_key
            st.success("âœ… APIã‚­ãƒ¼è¨­å®šæ¸ˆã¿")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¯”è¼ƒ")
        st.markdown("""
        **PubMed**
        - åŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦ç‰¹åŒ–
        - å…¬å¼APIãƒ»å®‰å®š
        - å¼•ç”¨æ•°ãªã—
        
        **Semantic Scholar** â­
        - å…¨åˆ†é‡å¯¾å¿œ
        - å¼•ç”¨æ•°ã‚ã‚Š
        - ç„¡æ–™ãƒ»å®‰å®š
        - Rate limit: 100req/5min
        
        **Google Scholar**
        - æœ€å¤§ã®DB
        - ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚„ã™ã„
        """)

    # ã‚¿ãƒ–
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "ğŸ“š è«–æ–‡æ¤œç´¢",
        "ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰",
        "ğŸ¤– AIä¸€æ‹¬è¦ç´„",
        "ğŸ“Š ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰",
        "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
        "ğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿"
    ])

    # ã‚¿ãƒ–1: è«–æ–‡æ¤œç´¢
    with tab1:
        st.header("è«–æ–‡æ¤œç´¢")

        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            ["PubMedï¼ˆåŒ»å­¦ãƒ»ç”Ÿå‘½ç§‘å­¦ï¼‰", "Semantic Scholarï¼ˆå…¨åˆ†é‡ãƒ»å¼•ç”¨æ•°ã‚ã‚Šï¼‰", "Google Scholarï¼ˆãƒ–ãƒ­ãƒƒã‚¯æ³¨æ„ï¼‰"],
            help="Semantic ScholarãŒæœ€ã‚‚ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ããŠã™ã™ã‚ã§ã™"
        )

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                placeholder="ä¾‹: mass spectrometry proteomics"
            )
        with col2:
            max_results = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=20, value=10)

        col3, col4 = st.columns(2)
        with col3:
            year_from = st.number_input("æ¤œç´¢é–‹å§‹å¹´", min_value=2000, max_value=2030, value=2015)
        with col4:
            search_mode = st.selectbox("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["é€šå¸¸æ¤œç´¢", "æœ€è¿‘ã®è«–æ–‡ï¼ˆç›´è¿‘7æ—¥ï¼‰"])

        if st.button("ğŸ” æ¤œç´¢é–‹å§‹", type="primary"):
            if not query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                with st.spinner(f"{data_source}ã§è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                    try:
                        if "PubMed" in data_source:
                            crawler = PubMedCrawler()
                        elif "Semantic Scholar" in data_source:
                            crawler = SemanticScholarCrawler()
                        else:
                            crawler = ScholarCrawler()

                        if search_mode == "é€šå¸¸æ¤œç´¢":
                            papers = crawler.search_papers(query, max_results, year_from)
                        else:
                            papers = crawler.get_recent_papers(query, days=7, max_results=max_results)

                        if papers:
                            st.session_state.papers = papers
                            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆã‚½ãƒ¼ã‚¹: {data_source}ï¼‰")

                            for i, paper in enumerate(papers[:10], 1):
                                with st.expander(f"ğŸ“„ {i}. {paper['title'][:80]}..."):
                                    authors_list = paper['authors']
                                    if isinstance(authors_list, list):
                                        authors_str = ', '.join(authors_list[:3])
                                        if len(authors_list) > 3:
                                            authors_str += f" ä»–{len(authors_list) - 3}å"
                                    else:
                                        authors_str = authors_list
                                    
                                    st.markdown(f"**è‘—è€…**: {authors_str}")
                                    st.markdown(f"**ç™ºè¡¨å¹´**: {paper['year']} | **ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**: {paper.get('venue', 'N/A')}")
                                    if paper.get('citations', 0) > 0:
                                        st.markdown(f"**å¼•ç”¨æ•°**: {paper['citations']}")
                                    st.markdown(f"**URL**: [{paper['url']}]({paper['url']})")
                                    if paper['abstract'] != 'N/A':
                                        st.markdown(f"**è¦æ—¨**: {paper['abstract'][:400]}...")
                        else:
                            st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    # ã‚¿ãƒ–2: ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
    with tab2:
        st.header("ğŸ“ˆ ç ”ç©¶ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")

        if st.session_state.papers:
            st.subheader("ğŸ“… å¹´æ¬¡è«–æ–‡æ•°æ¨ç§»")
            
            years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A' and str(p['year']).isdigit()]
            
            if years:
                year_counts = Counter(years)
                year_df = pd.DataFrame(
                    list(year_counts.items()),
                    columns=['å¹´', 'è«–æ–‡æ•°']
                ).sort_values('å¹´')
                
                fig, ax = plt.subplots(figsize=(12, 6))
                ax.plot(year_df['å¹´'], year_df['è«–æ–‡æ•°'], marker='o', linewidth=2, markersize=8)
                ax.set_xlabel('å¹´', fontsize=12)
                ax.set_ylabel('è«–æ–‡æ•°', fontsize=12)
                ax.set_title('å¹´æ¬¡è«–æ–‡æ•°æ¨ç§»', fontsize=14, fontweight='bold')
                ax.grid(True, alpha=0.3)
                st.pyplot(fig)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç·è«–æ–‡æ•°", len(years))
                with col2:
                    peak_year = year_counts.most_common(1)[0][0]
                    st.metric("ãƒ”ãƒ¼ã‚¯å¹´", peak_year)
                with col3:
                    avg_per_year = len(years) / len(year_counts) if year_counts else 0
                    st.metric("å¹´å¹³å‡", f"{avg_per_year:.1f}ä»¶")
                
            else:
                st.warning("å¹´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–3: AIä¸€æ‹¬è¦ç´„
    with tab3:
        st.header("ğŸ¤– AIä¸€æ‹¬è¦ç´„ï¼ˆGemini APIï¼‰")

        if st.session_state.papers:
            if not st.session_state.gemini_api_key:
                st.warning("âš ï¸ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Gemini API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„")
                st.markdown("[Google AI Studio](https://makersuite.google.com/app/apikey)ã§ç„¡æ–™ã§å–å¾—ã§ãã¾ã™")
            else:
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãŒã‚ã‚Šã¾ã™")
                with col2:
                    max_summarize = st.number_input("è¦ç´„ã™ã‚‹ä»¶æ•°", 1, min(10, len(st.session_state.papers)), 5)
                
                if st.button("ğŸ¤– AIè¦ç´„ã‚’é–‹å§‹", type="primary"):
                    papers_to_summarize = st.session_state.papers[:max_summarize]
                    
                    with st.spinner(f"{len(papers_to_summarize)}ä»¶ã®è«–æ–‡ã‚’è¦ç´„ä¸­..."):
                        summaries = summarize_papers_with_gemini(
                            papers_to_summarize,
                            st.session_state.gemini_api_key
                        )
                        
                        if summaries:
                            st.session_state.summaries = summaries
                            st.success(f"âœ… {len(summaries)}ä»¶ã®è«–æ–‡ã‚’è¦ç´„ã—ã¾ã—ãŸ")
                            
                            for i, (title, summary) in enumerate(summaries.items(), 1):
                                with st.expander(f"ğŸ“ {i}. {title[:70]}..."):
                                    st.markdown(f"**è¦ç´„**:")
                                    st.markdown(summary)
                                    
                                    original_paper = next((p for p in papers_to_summarize if p['title'] == title), None)
                                    if original_paper:
                                        st.markdown(f"**URL**: [{original_paper['url']}]({original_paper['url']})")
                        else:
                            st.error("è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–4-6ã¯å‰ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ãªã®ã§çœç•¥ï¼ˆæ–‡å­—æ•°åˆ¶é™ã®ãŸã‚ï¼‰
    # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ã§ã¯ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã€å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ãƒ–ã‚‚å«ã‚ã¦ãã ã•ã„


if __name__ == "__main__":
    main()
