"""
Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ©Ÿèƒ½æ‹¡å¼µç‰ˆï¼‰
- PubMed & Google Scholar å¯¾å¿œ
- ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ & å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ
"""
import streamlit as st
import sys
import os
from datetime import datetime, timedelta
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import time
import requests
from typing import List, Dict, Optional
from xml.etree import ElementTree as ET
import re
from itertools import combinations
import networkx as nx

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”¬",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
if 'papers' not in st.session_state:
    st.session_state.papers = []
if 'summaries' not in st.session_state:
    st.session_state.summaries = {}


# ==================== PubMed Crawler ====================
class PubMedCrawler:
    """PubMed APIã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—"""

    def __init__(self, email: str = "user@example.com"):
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        self.email = email

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'relevance'
            }

            if year_from:
                current_year = datetime.now().year
                search_params['datetype'] = 'pdat'
                search_params['mindate'] = f"{year_from}/01/01"
                search_params['maxdate'] = f"{current_year}/12/31"

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            batch_size = 5
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i + batch_size]
                ids_str = ','.join(batch_ids)

                fetch_params = {
                    'db': 'pubmed',
                    'id': ids_str,
                    'retmode': 'xml',
                    'email': self.email
                }

                fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
                fetch_response.raise_for_status()
                root = ET.fromstring(fetch_response.content)

                for article in root.findall('.//PubmedArticle'):
                    try:
                        paper_info = self._extract_paper_info(article, keyword)
                        papers.append(paper_info)
                    except:
                        continue
                time.sleep(0.5)

            return papers

        except Exception as e:
            st.error(f"PubMed API ã‚¨ãƒ©ãƒ¼: {e}")
            return papers

    def _extract_paper_info(self, article_xml, keyword: str) -> Dict:
        title_elem = article_xml.find('.//ArticleTitle')
        title = title_elem.text if title_elem is not None else 'N/A'

        authors = []
        for author in article_xml.findall('.//Author'):
            lastname = author.find('LastName')
            forename = author.find('ForeName')
            if lastname is not None:
                name = lastname.text
                if forename is not None:
                    name = f"{forename.text} {name}"
                authors.append(name)

        year_elem = article_xml.find('.//PubDate/Year')
        year = year_elem.text if year_elem is not None else 'N/A'

        abstract_texts = []
        for abstract in article_xml.findall('.//AbstractText'):
            if abstract.text:
                abstract_texts.append(abstract.text)
        abstract = ' '.join(abstract_texts) if abstract_texts else 'N/A'

        journal_elem = article_xml.find('.//Journal/Title')
        venue = journal_elem.text if journal_elem is not None else 'N/A'

        pmid_elem = article_xml.find('.//PMID')
        pmid = pmid_elem.text if pmid_elem is not None else ''
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else 'N/A'

        return {
            'title': title,
            'authors': authors,
            'year': year,
            'abstract': abstract,
            'venue': venue,
            'url': url,
            'pmid': pmid,
            'citations': 0,
            'crawled_at': datetime.now().isoformat(),
            'keyword': keyword,
            'source': 'PubMed'
        }

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        papers = []
        try:
            search_url = f"{self.base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': keyword,
                'retmax': max_results,
                'retmode': 'json',
                'email': self.email,
                'sort': 'date',
                'reldate': days
            }

            search_response = requests.get(search_url, params=search_params, timeout=10)
            search_response.raise_for_status()
            search_data = search_response.json()

            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            if not id_list:
                return papers

            fetch_url = f"{self.base_url}efetch.fcgi"
            ids_str = ','.join(id_list)

            fetch_params = {
                'db': 'pubmed',
                'id': ids_str,
                'retmode': 'xml',
                'email': self.email
            }

            fetch_response = requests.get(fetch_url, params=fetch_params, timeout=10)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.content)

            for article in root.findall('.//PubmedArticle'):
                try:
                    paper_info = self._extract_paper_info(article, keyword)
                    papers.append(paper_info)
                except:
                    continue

            return papers

        except Exception as e:
            st.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return papers


# ==================== Google Scholar Crawler ====================
class ScholarCrawler:
    """Google Scholarã‹ã‚‰è«–æ–‡æƒ…å ±ã‚’å–å¾—ï¼ˆscholarlyä½¿ç”¨ï¼‰"""

    def __init__(self):
        self.results = []
        # scholarly ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é…å»¶å®Ÿè¡Œ
        try:
            from scholarly import scholarly, ProxyGenerator
            self.scholarly = scholarly
            
            # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šï¼ˆãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ï¼‰
            try:
                pg = ProxyGenerator()
                pg.FreeProxies()
                scholarly.use_proxy(pg)
            except:
                pass  # ãƒ—ãƒ­ã‚­ã‚·è¨­å®šå¤±æ•—ã—ã¦ã‚‚ã‚¹ã‚­ãƒƒãƒ—
        except ImportError:
            st.error("scholarly ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            self.scholarly = None

    def search_papers(
        self,
        keyword: str,
        max_results: int = 20,
        year_from: Optional[int] = None
    ) -> List[Dict]:
        if not self.scholarly:
            return []

        papers = []
        try:
            search_query = keyword
            if year_from:
                search_query = f"{keyword} after:{year_from}"

            search_results = self.scholarly.search_pubs(search_query)

            count = 0
            for result in search_results:
                if count >= max_results:
                    break

                try:
                    paper_info = {
                        'title': result.get('bib', {}).get('title', 'N/A'),
                        'authors': result.get('bib', {}).get('author', []),
                        'year': result.get('bib', {}).get('pub_year', 'N/A'),
                        'abstract': result.get('bib', {}).get('abstract', 'N/A'),
                        'venue': result.get('bib', {}).get('venue', 'N/A'),
                        'url': result.get('pub_url', result.get('eprint_url', 'N/A')),
                        'citations': result.get('num_citations', 0),
                        'crawled_at': datetime.now().isoformat(),
                        'keyword': keyword,
                        'source': 'Google Scholar'
                    }

                    papers.append(paper_info)
                    count += 1
                    time.sleep(2)  # Rate limitå¯¾ç­–

                except Exception as e:
                    continue

            return papers

        except Exception as e:
            st.error(f"Google Scholar ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ’¡ Google ScholarãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚PubMedã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
            return papers

    def get_recent_papers(self, keyword: str, days: int = 7, max_results: int = 20) -> List[Dict]:
        current_year = datetime.now().year
        target_date = datetime.now() - timedelta(days=days)
        year_from = target_date.year
        return self.search_papers(keyword, max_results, year_from)


# ==================== ãƒ†ã‚­ã‚¹ãƒˆè§£æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====================
def extract_keywords(text: str, min_length: int = 4, top_n: int = 50) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    # è‹±å˜èªã®ã¿æŠ½å‡ºï¼ˆå°æ–‡å­—åŒ–ï¼‰
    words = re.findall(r'\b[a-zA-Z]{' + str(min_length) + r',}\b', text.lower())
    
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
    stop_words = {
        'this', 'that', 'with', 'from', 'were', 'been', 'have', 'has', 'had',
        'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can',
        'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can',
        'was', 'said', 'them', 'been', 'than', 'find', 'also', 'made',
        'when', 'what', 'which', 'their', 'these', 'those', 'such', 'into',
        'through', 'during', 'before', 'after', 'about', 'between', 'under'
    }
    
    filtered_words = [w for w in words if w not in stop_words]
    
    # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter(filtered_words)
    return [word for word, _ in word_counts.most_common(top_n)]


def build_cooccurrence_network(papers: List[Dict], top_keywords: int = 30, window_size: int = 10):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""
    # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    all_text = " ".join([
        f"{p['title']} {p['abstract']}"
        for p in papers
        if p['abstract'] != 'N/A'
    ])
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = extract_keywords(all_text, min_length=5, top_n=top_keywords)
    
    # å…±èµ·è¡Œåˆ—ã‚’ä½œæˆ
    cooccurrence = Counter()
    
    for paper in papers:
        text = f"{paper['title']} {paper['abstract']}"
        words = re.findall(r'\b[a-zA-Z]{5,}\b', text.lower())
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã§ã®å…±èµ·ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for i, word1 in enumerate(words):
            if word1 not in keywords:
                continue
            
            for j in range(i + 1, min(i + window_size, len(words))):
                word2 = words[j]
                if word2 in keywords and word1 != word2:
                    pair = tuple(sorted([word1, word2]))
                    cooccurrence[pair] += 1
    
    return keywords, cooccurrence


# ==================== ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª ====================
def main():
    st.title("ğŸ”¬ Mass Spectrometry è«–æ–‡ç ”ç©¶ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("PubMedãƒ»Google Scholarã‹ã‚‰è«–æ–‡ã‚’æ¤œç´¢ã—ã€é«˜åº¦ãªè§£æãŒå¯èƒ½ãªã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("---")

    # ã‚¿ãƒ–
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“š è«–æ–‡æ¤œç´¢", "ğŸ“Š ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ’¾ ä¿å­˜ãƒ‡ãƒ¼ã‚¿"])

    # ã‚¿ãƒ–1: è«–æ–‡æ¤œç´¢
    with tab1:
        st.header("è«–æ–‡æ¤œç´¢")

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            ["PubMedï¼ˆæ¨å¥¨ãƒ»å®‰å®šï¼‰", "Google Scholar"],
            help="PubMedã¯å…¬å¼APIã§å®‰å®šã€‚Google Scholarã¯å¼•ç”¨æ•°ã‚‚å–å¾—ã§ãã‚‹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Š"
        )

        col1, col2 = st.columns([3, 1])
        with col1:
            query = st.text_input(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
                placeholder="ä¾‹: mass spectrometry proteomics",
                help="æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›"
            )
        with col2:
            max_results = st.number_input("å–å¾—ä»¶æ•°", min_value=1, max_value=50, value=10)

        col3, col4 = st.columns(2)
        with col3:
            year_from = st.number_input("æ¤œç´¢é–‹å§‹å¹´", min_value=2000, max_value=2030, value=2020)
        with col4:
            search_mode = st.selectbox("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰", ["é€šå¸¸æ¤œç´¢", "æœ€è¿‘ã®è«–æ–‡ï¼ˆç›´è¿‘7æ—¥ï¼‰"])

        if st.button("ğŸ” æ¤œç´¢é–‹å§‹", type="primary"):
            if not query:
                st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                with st.spinner(f"{data_source}ã§è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
                    try:
                        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«å¿œã˜ã¦ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’é¸æŠ
                        if data_source == "PubMedï¼ˆæ¨å¥¨ãƒ»å®‰å®šï¼‰":
                            crawler = PubMedCrawler()
                        else:
                            crawler = ScholarCrawler()

                        if search_mode == "é€šå¸¸æ¤œç´¢":
                            papers = crawler.search_papers(query, max_results, year_from)
                        else:
                            papers = crawler.get_recent_papers(query, days=7, max_results=max_results)

                        if papers:
                            st.session_state.papers = papers
                            st.success(f"âœ… {len(papers)}ä»¶ã®è«–æ–‡ã‚’å–å¾—ã—ã¾ã—ãŸï¼ˆã‚½ãƒ¼ã‚¹: {data_source}ï¼‰")

                            # çµæœã‚’è¡¨ç¤º
                            for i, paper in enumerate(papers[:10], 1):
                                with st.expander(f"ğŸ“„ {i}. {paper['title'][:80]}..."):
                                    authors_list = paper['authors']
                                    if isinstance(authors_list, list):
                                        authors_str = ', '.join(authors_list[:3])
                                        if len(authors_list) > 3:
                                            authors_str += f" ä»–{len(authors_list) - 3}å"
                                    else:
                                        authors_str = authors_list
                                    
                                    st.markdown(f"**è‘—è€…**: {authors_str}")
                                    st.markdown(f"**ç™ºè¡¨å¹´**: {paper['year']} | **ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«**: {paper.get('venue', 'N/A')}")
                                    if paper.get('citations', 0) > 0:
                                        st.markdown(f"**å¼•ç”¨æ•°**: {paper['citations']}")
                                    st.markdown(f"**URL**: [{paper['url']}]({paper['url']})")
                                    if paper['abstract'] != 'N/A':
                                        st.markdown(f"**è¦æ—¨**: {paper['abstract'][:400]}...")
                        else:
                            st.warning("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                        if data_source == "Google Scholar":
                            st.info("ğŸ’¡ Google Scholarã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸå ´åˆã¯ã€PubMedã‚’ãŠè©¦ã—ãã ã•ã„")

    # ã‚¿ãƒ–2: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰
    with tab2:
        st.header("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ")

        if st.session_state.papers:
            col1, col2 = st.columns([3, 1])
            with col1:
                st.info(f"ç¾åœ¨ {len(st.session_state.papers)} ä»¶ã®è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã™")
            with col2:
                max_words = st.slider("æœ€å¤§å˜èªæ•°", 30, 200, 100)

            if st.button("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ"):
                with st.spinner("ç”Ÿæˆä¸­..."):
                    text = " ".join([
                        f"{p['title']} {p['abstract']}"
                        for p in st.session_state.papers
                        if p['abstract'] != 'N/A'
                    ])

                    if text:
                        wordcloud = WordCloud(
                            width=1200,
                            height=600,
                            background_color='white',
                            colormap='viridis',
                            max_words=max_words
                        ).generate(text)

                        fig, ax = plt.subplots(figsize=(15, 7))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        st.success("âœ… ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–3: å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    with tab3:
        st.header("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è§£æ")
        st.markdown("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®é–¢ä¿‚æ€§ã‚’å¯è¦–åŒ–ã—ã¾ã™")

        if st.session_state.papers:
            col1, col2, col3 = st.columns(3)
            with col1:
                top_keywords = st.slider("è¡¨ç¤ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", 10, 50, 30)
            with col2:
                window_size = st.slider("å…±èµ·ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦", 5, 20, 10)
            with col3:
                min_cooccurrence = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 10, 2)

            if st.button("ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆ"):
                with st.spinner("è§£æä¸­..."):
                    keywords, cooccurrence = build_cooccurrence_network(
                        st.session_state.papers,
                        top_keywords=top_keywords,
                        window_size=window_size
                    )

                    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
                    G = nx.Graph()
                    
                    for (word1, word2), count in cooccurrence.items():
                        if count >= min_cooccurrence:
                            G.add_edge(word1, word2, weight=count)

                    if len(G.nodes()) > 0:
                        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—
                        pos = nx.spring_layout(G, k=0.5, iterations=50)

                        # æç”»
                        fig, ax = plt.subplots(figsize=(16, 12))
                        
                        # ãƒãƒ¼ãƒ‰æç”»
                        node_sizes = [G.degree(node) * 300 for node in G.nodes()]
                        nx.draw_networkx_nodes(
                            G, pos,
                            node_size=node_sizes,
                            node_color='lightblue',
                            alpha=0.7,
                            ax=ax
                        )

                        # ã‚¨ãƒƒã‚¸æç”»
                        edges = G.edges()
                        weights = [G[u][v]['weight'] for u, v in edges]
                        max_weight = max(weights) if weights else 1
                        
                        nx.draw_networkx_edges(
                            G, pos,
                            width=[w / max_weight * 5 for w in weights],
                            alpha=0.3,
                            ax=ax
                        )

                        # ãƒ©ãƒ™ãƒ«æç”»
                        nx.draw_networkx_labels(
                            G, pos,
                            font_size=10,
                            font_weight='bold',
                            ax=ax
                        )

                        ax.axis('off')
                        ax.set_title(
                            f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (ãƒãƒ¼ãƒ‰æ•°: {len(G.nodes())}, ã‚¨ãƒƒã‚¸æ•°: {len(G.edges())})",
                            fontsize=16
                        )
                        
                        st.pyplot(fig)
                        
                        # çµ±è¨ˆæƒ…å ±
                        st.subheader("ğŸ“Š ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ãƒãƒ¼ãƒ‰æ•°ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰", len(G.nodes()))
                        with col2:
                            st.metric("ã‚¨ãƒƒã‚¸æ•°ï¼ˆå…±èµ·é–¢ä¿‚ï¼‰", len(G.edges()))
                        with col3:
                            if len(G.nodes()) > 0:
                                avg_degree = sum(dict(G.degree()).values()) / len(G.nodes())
                                st.metric("å¹³å‡æ¬¡æ•°", f"{avg_degree:.2f}")

                        # ä¸­å¿ƒæ€§ã®é«˜ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                        st.subheader("ğŸ¯ é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸­å¿ƒæ€§é †ï¼‰")
                        if len(G.nodes()) > 0:
                            degree_centrality = nx.degree_centrality(G)
                            top_central = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]
                            
                            df_central = pd.DataFrame(top_central, columns=['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', 'ä¸­å¿ƒæ€§'])
                            st.dataframe(df_central, use_container_width=True)

                        st.success("âœ… å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆå®Œäº†")
                    else:
                        st.warning("å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€å°å…±èµ·å›æ•°ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚")

        else:
            st.info("ã¾ãšã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã§è«–æ–‡ã‚’å–å¾—ã—ã¦ãã ã•ã„")

    # ã‚¿ãƒ–4: ä¿å­˜ãƒ‡ãƒ¼ã‚¿
    with tab4:
        st.header("ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿")

        if st.session_state.papers:
            st.subheader("ğŸ“š å–å¾—æ¸ˆã¿è«–æ–‡")
            
            df_data = []
            for p in st.session_state.papers:
                authors_list = p['authors']
                if isinstance(authors_list, list):
                    authors_str = ', '.join(authors_list[:2])
                    if len(authors_list) > 2:
                        authors_str += '...'
                else:
                    authors_str = authors_list[:50]
                
                df_data.append({
                    'ã‚¿ã‚¤ãƒˆãƒ«': p['title'][:60] + '...' if len(p['title']) > 60 else p['title'],
                    'è‘—è€…': authors_str,
                    'å¹´': p['year'],
                    'ã‚½ãƒ¼ã‚¹': p.get('source', 'N/A'),
                    'å¼•ç”¨': p.get('citations', 0)
                })
            
            df = pd.DataFrame(df_data)
            st.dataframe(df, use_container_width=True)

            # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = pd.DataFrame(st.session_state.papers).to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name=f"papers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

            # çµ±è¨ˆæƒ…å ±
            st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ç·è«–æ–‡æ•°", len(st.session_state.papers))
            
            with col2:
                years = [p['year'] for p in st.session_state.papers if p['year'] != 'N/A']
                if years:
                    year_counts = Counter(years)
                    most_common_year = year_counts.most_common(1)[0][0]
                    st.metric("æœ€å¤šç™ºè¡¨å¹´", most_common_year)
            
            with col3:
                if years:
                    st.metric("æœ€æ–°å¹´", max(years))
            
            with col4:
                total_citations = sum([p.get('citations', 0) for p in st.session_state.papers])
                st.metric("ç·å¼•ç”¨æ•°", total_citations)

            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥é›†è¨ˆ
            st.subheader("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥")
            sources = [p.get('source', 'Unknown') for p in st.session_state.papers]
            source_counts = Counter(sources)
            
            df_sources = pd.DataFrame(source_counts.items(), columns=['ã‚½ãƒ¼ã‚¹', 'è«–æ–‡æ•°'])
            st.bar_chart(df_sources.set_index('ã‚½ãƒ¼ã‚¹'))

        else:
            st.info("ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã€Œè«–æ–‡æ¤œç´¢ã€ã‚¿ãƒ–ã‹ã‚‰å§‹ã‚ã¦ãã ã•ã„ã€‚")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ“– ä½¿ã„æ–¹")
    st.sidebar.markdown("""
    **1. è«–æ–‡æ¤œç´¢**
    - PubMed/Google Scholarã‚’é¸æŠ
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢
    
    **2. ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰**
    - é »å‡ºå˜èªã‚’è¦–è¦šåŒ–
    
    **3. å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**
    - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®é–¢ä¿‚æ€§ã‚’è§£æ
    - ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º = é‡è¦åº¦
    - ã‚¨ãƒƒã‚¸ã®å¤ªã• = å…±èµ·é »åº¦
    
    **4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜**
    - CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½
    """)
    st.sidebar.markdown("---")
    st.sidebar.info("ğŸ’¡ ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã®ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¾ã™")


if __name__ == "__main__":
    main()
